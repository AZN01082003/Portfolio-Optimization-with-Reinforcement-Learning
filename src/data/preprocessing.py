"""
Module pour le pr√©traitement des donn√©es financi√®res.
Version corrig√©e avec compatibilit√© environnement RL et gestion d'erreurs robuste.
"""
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import logging
from datetime import datetime
import json
from typing import Tuple, Optional, List, Dict, Any

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def load_config(config_path: str = "config/default.json") -> dict:
    """
    Charge la configuration depuis un fichier json.
    
    Args:
        config_path: Chemin du fichier de configuration
        
    Returns:
        dict: Configuration charg√©e
    """
    try:
        with open(config_path, 'r', encoding='utf-8') as file:
            config = json.load(file)
        logger.info(f"Configuration charg√©e depuis: {config_path}")
        return config
    except Exception as e:
        logger.error(f"Erreur lors du chargement de la configuration: {e}")
        raise



# Dans src/data/preprocessing.py, ajoutez cette fonction avant detect_data_format :

def load_csv_data(filepath: str) -> pd.DataFrame:
    """
    Charge un fichier CSV en d√©tectant automatiquement s'il s'agit d'un MultiIndex.
    
    Args:
        filepath: Chemin vers le fichier CSV
        
    Returns:
        pd.DataFrame: DataFrame charg√© avec la structure appropri√©e
    """
    try:
        # Essayer de charger avec MultiIndex
        df_multiindex = pd.read_csv(filepath, index_col=0, header=[0, 1], parse_dates=True)
        
        # V√©rifier si c'est vraiment un MultiIndex valide
        if (isinstance(df_multiindex.columns, pd.MultiIndex) and 
            not any('Unnamed:' in str(col) for col in df_multiindex.columns.get_level_values(0))):
            logger.info("Fichier CSV charg√© comme MultiIndex")
            return df_multiindex
    except:
        pass
    
    try:
        # Essayer de charger normalement
        df_normal = pd.read_csv(filepath, index_col=0, parse_dates=True)
        logger.info("Fichier CSV charg√© comme DataFrame normal")
        return df_normal
    except Exception as e:
        logger.error(f"Erreur lors du chargement du CSV: {e}")
        raise



def detect_data_format(data: pd.DataFrame) -> Dict[str, Any]:
    """
    D√©tecte le format des donn√©es d'entr√©e.
    
    Args:
        data: DataFrame √† analyser
        
    Returns:
        Dict contenant les informations sur le format
    """
    format_info = {
        'is_multiindex': isinstance(data.columns, pd.MultiIndex),
        'n_columns': len(data.columns),
        'n_rows': len(data),
        'has_ticker_column': 'ticker' in data.columns,
        'column_names': list(data.columns),
        'index_type': type(data.index).__name__
    }
    
    # D√©tection des features financi√®res standard
    standard_features = ['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']
    format_info['detected_features'] = []
    
    if format_info['is_multiindex']:
        # Format MultiIndex (feature, ticker)
        level_0_names = data.columns.get_level_values(0).unique().tolist()
        level_1_names = data.columns.get_level_values(1).unique().tolist()
        format_info['features'] = level_0_names
        format_info['tickers'] = level_1_names
        format_info['detected_features'] = [f for f in standard_features if f in level_0_names]
    else:
        # Format simple ou avec colonne ticker
        if format_info['has_ticker_column']:
            # Format long avec colonne ticker
            format_info['features'] = [col for col in data.columns if col != 'ticker']
            format_info['tickers'] = data['ticker'].unique().tolist() if 'ticker' in data.columns else []
        else:
            # Format simple (un seul ticker)
            format_info['features'] = list(data.columns)
            format_info['tickers'] = ['SINGLE_TICKER']
        
        format_info['detected_features'] = [f for f in standard_features if f in format_info['features']]
    
    logger.info(f"Format d√©tect√©: {format_info}")
    return format_info

def preprocess_data(data: pd.DataFrame, tickers: List[str]) -> np.ndarray:
    """
    Pr√©traite les donn√©es t√©l√©charg√©es pour les rendre compatibles avec l'environnement RL.
    
    Args:
        data: DataFrame des donn√©es financi√®res
        tickers: Liste des symboles d'actions
    
    Returns:
        np.ndarray: Donn√©es pr√©trait√©es de forme (n_features, n_stocks, n_time_periods)
    """
    try:
        # Analyser le format des donn√©es
        format_info = detect_data_format(data)
        
        # D√©finir les features √† extraire (dans l'ordre pour l'environnement RL)
        target_features = ['Open', 'High', 'Low', 'Close', 'Volume']
        
        # Initialiser le tableau r√©sultat
        n_features = len(target_features)
        n_stocks = len(tickers)
        n_time_periods = len(data)
        
        result = np.zeros((n_features, n_stocks, n_time_periods), dtype=np.float32)
        
        logger.info(f"Pr√©traitement: {n_features} features, {n_stocks} stocks, {n_time_periods} p√©riodes")
        
        if format_info['is_multiindex']:
            # Traitement donn√©es MultiIndex
            logger.info("Traitement des donn√©es MultiIndex")
            
            for i, feature in enumerate(target_features):
                for j, ticker in enumerate(tickers):
                    if (feature, ticker) in data.columns:
                        values = data[(feature, ticker)].values
                        result[i, j, :] = values
                    elif feature == 'Volume' and ('Vol', ticker) in data.columns:
                        # Gestion alternative pour Volume
                        values = data[('Vol', ticker)].values
                        result[i, j, :] = values
                    elif feature == 'Close' and ('Adj Close', ticker) in data.columns:
                        # Utiliser Adj Close si Close n'est pas disponible
                        values = data[('Adj Close', ticker)].values
                        result[i, j, :] = values
                    else:
                        # Utiliser une valeur par d√©faut ou interpoler
                        if i < 4:  # Prix (OHLC)
                            # Essayer d'utiliser Close comme fallback
                            if ('Close', ticker) in data.columns:
                                values = data[('Close', ticker)].values
                                result[i, j, :] = values
                            else:
                                logger.warning(f"Feature {feature} non trouv√©e pour {ticker}, utilisation de valeurs par d√©faut")
                                result[i, j, :] = 100.0  # Prix par d√©faut
                        else:  # Volume
                            logger.warning(f"Volume non trouv√© pour {ticker}, utilisation de valeurs par d√©faut")
                            result[i, j, :] = 1000000  # Volume par d√©faut
        
        elif format_info['has_ticker_column']:
            # Traitement donn√©es format long avec colonne ticker
            logger.info("Traitement des donn√©es format long")
            
            for j, ticker in enumerate(tickers):
                ticker_data = data[data['ticker'] == ticker]
                if len(ticker_data) == 0:
                    logger.warning(f"Aucune donn√©e trouv√©e pour {ticker}")
                    # Remplir avec des valeurs par d√©faut
                    for i in range(n_features):
                        if i < 4:
                            result[i, j, :] = 100.0 + i * 10  # Prix diff√©rents OHLC
                        else:
                            result[i, j, :] = 1000000  # Volume
                    continue
                
                # R√©indexer pour s'assurer d'avoir toutes les p√©riodes
                ticker_data = ticker_data.set_index(ticker_data.index).reindex(data.index)
                
                for i, feature in enumerate(target_features):
                    if feature in ticker_data.columns:
                        values = ticker_data[feature].values
                        result[i, j, :] = values
                    else:
                        logger.warning(f"Feature {feature} non trouv√©e pour {ticker}")
                        if i < 4:
                            result[i, j, :] = 100.0
                        else:
                            result[i, j, :] = 1000000
        
        else:
            # Traitement donn√©es simple (un seul ticker)
            logger.info("Traitement des donn√©es simple ticker")
            
            if len(tickers) > 1:
                logger.warning("Plusieurs tickers demand√©s mais donn√©es pour un seul ticker")
                # Dupliquer les donn√©es pour tous les tickers
                for j, ticker in enumerate(tickers):
                    for i, feature in enumerate(target_features):
                        if feature in data.columns:
                            values = data[feature].values
                            result[i, j, :] = values
                        else:
                            if i < 4:
                                result[i, j, :] = 100.0
                            else:
                                result[i, j, :] = 1000000
            else:
                # Un seul ticker
                for i, feature in enumerate(target_features):
                    if feature in data.columns:
                        values = data[feature].values
                        result[i, 0, :] = values
                    else:
                        if i < 4:
                            result[i, 0, :] = 100.0
                        else:
                            result[i, 0, :] = 1000000
        
        # V√©rifications et nettoyage
        result = clean_data(result)
        
        logger.info(f"Pr√©traitement termin√©. Forme finale: {result.shape}")
        return result
        
    except Exception as e:
        logger.error(f"Erreur lors du pr√©traitement: {e}")
        raise

def clean_data(data: np.ndarray) -> np.ndarray:
    """
    Nettoie les donn√©es (NaN, valeurs aberrantes, etc.).
    
    Args:
        data: Donn√©es √† nettoyer
        
    Returns:
        np.ndarray: Donn√©es nettoy√©es
    """
    logger.info("Nettoyage des donn√©es...")
    
    # Remplacer les NaN et les inf
    data = np.nan_to_num(data, nan=0.0, posinf=0.0, neginf=0.0)
    
    # Remplacer les z√©ros dans les prix par interpolation
    for feature_idx in range(min(4, data.shape[0])):  # OHLC seulement
        for stock_idx in range(data.shape[1]):
            series = data[feature_idx, stock_idx, :]
            
            # Trouver les z√©ros et les remplacer
            zero_mask = (series <= 0)
            if np.any(zero_mask):
                # Utiliser la valeur pr√©c√©dente ou suivante non nulle
                non_zero_indices = np.where(~zero_mask)[0]
                if len(non_zero_indices) > 0:
                    # Interpolation simple
                    for idx in np.where(zero_mask)[0]:
                        # Trouver la valeur pr√©c√©dente non nulle
                        prev_idx = non_zero_indices[non_zero_indices < idx]
                        next_idx = non_zero_indices[non_zero_indices > idx]
                        
                        if len(prev_idx) > 0 and len(next_idx) > 0:
                            # Interpolation lin√©aire
                            prev_val = series[prev_idx[-1]]
                            next_val = series[next_idx[0]]
                            series[idx] = (prev_val + next_val) / 2
                        elif len(prev_idx) > 0:
                            series[idx] = series[prev_idx[-1]]
                        elif len(next_idx) > 0:
                            series[idx] = series[next_idx[0]]
                        else:
                            series[idx] = 100.0  # Valeur par d√©faut
                else:
                    # Aucune valeur non nulle, utiliser valeur par d√©faut
                    series[:] = 100.0
                
                data[feature_idx, stock_idx, :] = series
    
    # S'assurer que les volumes sont positifs
    if data.shape[0] >= 5:  # Si on a un index Volume
        data[4, :, :] = np.maximum(data[4, :, :], 1000)  # Volume minimum de 1000
    
    logger.info("Nettoyage termin√©")
    return data

def normalize_data(data: np.ndarray) -> np.ndarray:
    """
    Normalise les donn√©es pour am√©liorer l'apprentissage.
    
    Args:
        data: Donn√©es pr√©trait√©es
    
    Returns:
        np.ndarray: Donn√©es normalis√©es
    """
    logger.info("Normalisation des donn√©es...")
    
    normalized_data = np.copy(data).astype(np.float32)
    
    # Normaliser les prix (OHLC) - indices 0-3
    for i in range(min(4, data.shape[0])):
        for j in range(data.shape[1]):
            series = normalized_data[i, j, :]
            
            # Trouver le premier prix valide
            first_valid_idx = 0
            while (first_valid_idx < len(series) and 
                   (series[first_valid_idx] <= 0 or np.isnan(series[first_valid_idx]))):
                first_valid_idx += 1
            
            if first_valid_idx < len(series):
                reference_price = series[first_valid_idx]
                if reference_price > 0:
                    # Normalisation relative au premier prix valide
                    normalized_data[i, j, :] = series / reference_price
                else:
                    normalized_data[i, j, :] = 1.0
            else:
                # Aucun prix valide trouv√©
                normalized_data[i, j, :] = 1.0
    
    # Normaliser le volume (indice 4) si pr√©sent
    if data.shape[0] >= 5:
        for j in range(data.shape[1]):
            volume_series = normalized_data[4, j, :]
            max_volume = np.max(volume_series)
            if max_volume > 0:
                normalized_data[4, j, :] = volume_series / max_volume
            else:
                normalized_data[4, j, :] = 0.1  # Volume normalis√© par d√©faut
    
    logger.info("Normalisation termin√©e")
    return normalized_data

def visualize_data(data: np.ndarray, tickers: List[str], feature_idx: int = 3, save_path: Optional[str] = None):
    """
    Visualise les donn√©es pour une caract√©ristique sp√©cifique.
    
    Args:
        data: Donn√©es normalis√©es
        tickers: Liste des symboles d'actions
        feature_idx: Index de la caract√©ristique (0=Open, 1=High, 2=Low, 3=Close, 4=Volume)
        save_path: Chemin pour sauvegarder le graphique (optionnel)
    """
    try:
        features = ['Open', 'High', 'Low', 'Close', 'Volume']
        feature = features[min(feature_idx, len(features) - 1)]
        
        plt.figure(figsize=(14, 8))
        
        for j, ticker in enumerate(tickers):
            if j < data.shape[1]:
                plt.plot(data[feature_idx, j, :], label=ticker, linewidth=2)
        
        plt.title(f"√âvolution de {feature} pour les actions s√©lectionn√©es", fontsize=14)
        plt.xlabel("P√©riode de temps", fontsize=12)
        plt.ylabel(f"{feature} (normalis√©)", fontsize=12)
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"Graphique sauvegard√© dans {save_path}")
        else:
            plt.show()
        
        plt.close()
        
    except Exception as e:
        logger.error(f"Erreur lors de la visualisation: {e}")

def split_data(data: np.ndarray, train_ratio: float = 0.7) -> Tuple[np.ndarray, np.ndarray]:
    """
    S√©pare les donn√©es en ensembles d'entra√Ænement et de test.
    
    Args:
        data: Donn√©es normalis√©es
        train_ratio: Ratio pour la division entra√Ænement/test
    
    Returns:
        Tuple[np.ndarray, np.ndarray]: (train_data, test_data)
    """
    if not 0 < train_ratio < 1:
        raise ValueError(f"train_ratio doit √™tre entre 0 et 1, obtenu: {train_ratio}")
    
    n_time_periods = data.shape[2]
    split_point = int(n_time_periods * train_ratio)
    
    # S'assurer qu'on a au moins quelques points dans chaque ensemble
    split_point = max(10, min(split_point, n_time_periods - 10))
    
    train_data = data[:, :, :split_point].copy()
    test_data = data[:, :, split_point:].copy()
    
    logger.info(f"Division des donn√©es - Entra√Ænement: {train_data.shape}, Test: {test_data.shape}")
    return train_data, test_data

def save_data(data: np.ndarray, filepath: str, metadata: Optional[Dict] = None):
    """
    Enregistre les donn√©es pr√©trait√©es avec m√©tadonn√©es.
    
    Args:
        data: Donn√©es √† sauvegarder
        filepath: Chemin du fichier de sortie
        metadata: M√©tadonn√©es optionnelles
    """
    try:
        # S'assurer que le r√©pertoire existe
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        # Sauvegarder les donn√©es
        np.save(filepath, data)
        logger.info(f"Donn√©es enregistr√©es dans {filepath}")
        logger.info(f"Forme des donn√©es: {data.shape}")
        
        # Sauvegarder les m√©tadonn√©es si fournies
        if metadata:
            metadata_path = filepath.replace('.npy', '_metadata.json')
            metadata_to_save = {
                'shape': list(data.shape),
                'dtype': str(data.dtype),
                'created_at': datetime.now().isoformat(),
                **metadata
            }
            
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata_to_save, f, indent=2, default=str)
            logger.info(f"M√©tadonn√©es sauvegard√©es dans {metadata_path}")
            
    except Exception as e:
        logger.error(f"Erreur lors de la sauvegarde: {e}")
        raise


# Et modifiez la fonction main pour utiliser cette nouvelle fonction :

def main(config_path: str = "config/default.json", 
         use_existing_data: bool = False, 
         data_path: Optional[str] = None) -> Tuple[Optional[np.ndarray], Optional[np.ndarray], Optional[np.ndarray]]:
    """
    Fonction principale pour pr√©parer les donn√©es.
    
    Args:
        config_path: Chemin du fichier de configuration
        use_existing_data: Si True, utilise des donn√©es existantes
        data_path: Chemin vers les donn√©es existantes
    
    Returns:
        Tuple[np.ndarray, np.ndarray, np.ndarray]: (normalized_data, train_data, test_data)
    """
    try:
        # Charger la configuration
        config = load_config(config_path)
        
        # Extraire les param√®tres
        output_dir = config['data']['output_dir']
        train_ratio = config['data']['train_ratio']
        tickers = config['data']['tickers']
        
        # Cr√©er le r√©pertoire de sortie
        os.makedirs(output_dir, exist_ok=True)
        
        # Obtenir les donn√©es
        if use_existing_data and data_path and os.path.exists(data_path):
            logger.info(f"Utilisation des donn√©es existantes depuis {data_path}")
            data = load_csv_data(data_path)  # Utiliser la nouvelle fonction
        else:
            # Importer et utiliser l'ingestion
            from src.data.ingestion import main as ingest_data
            data, actual_tickers, _ = ingest_data(config_path)
            tickers = actual_tickers  # Utiliser les tickers effectivement r√©cup√©r√©s
        
        if data is None or data.empty:
            logger.error("Aucune donn√©e disponible pour le pr√©traitement")
            return None, None, None
        
        # Pr√©traiter les donn√©es
        logger.info("D√©but du pr√©traitement...")
        processed_data = preprocess_data(data, tickers)
        
        # Normaliser les donn√©es
        logger.info("D√©but de la normalisation...")
        normalized_data = normalize_data(processed_data)
        
        # Visualiser les donn√©es
        viz_path = os.path.join(output_dir, "price_evolution.png")
        visualize_data(normalized_data, tickers, save_path=viz_path)
        
        # Diviser en ensembles d'entra√Ænement et de test
        logger.info("Division des donn√©es...")
        train_data, test_data = split_data(normalized_data, train_ratio)
        
        # Pr√©parer les m√©tadonn√©es
        metadata = {
            'tickers': tickers,
            'n_features': normalized_data.shape[0],
            'n_stocks': normalized_data.shape[1],
            'n_time_periods': normalized_data.shape[2],
            'train_ratio': train_ratio,
            'features': ['Open', 'High', 'Low', 'Close', 'Volume'][:normalized_data.shape[0]]
        }
        
        # Sauvegarder les donn√©es
        timestamp = datetime.now().strftime("%Y%m%d")
        
        save_data(normalized_data, 
                 os.path.join(output_dir, f'stock_data_normalized_{timestamp}.npy'),
                 metadata)
        
        save_data(train_data, 
                 os.path.join(output_dir, f'stock_data_train_{timestamp}.npy'),
                 {**metadata, 'data_type': 'train'})
        
        save_data(test_data, 
                 os.path.join(output_dir, f'stock_data_test_{timestamp}.npy'),
                 {**metadata, 'data_type': 'test'})
        
        # Cr√©er des liens symboliques vers les derni√®res donn√©es
        for src, dst in [
            (f'stock_data_normalized_{timestamp}.npy', 'stock_data_normalized_latest.npy'),
            (f'stock_data_train_{timestamp}.npy', 'stock_data_train_latest.npy'),
            (f'stock_data_test_{timestamp}.npy', 'stock_data_test_latest.npy')
        ]:
            src_path = os.path.join(output_dir, src)
            dst_path = os.path.join(output_dir, dst)
            
            # Supprimer le lien existant s'il existe
            if os.path.exists(dst_path):
                os.remove(dst_path)
            
            # Cr√©er le nouveau lien (copie sur Windows)
            if os.name == 'nt':  # Windows
                import shutil
                shutil.copy2(src_path, dst_path)
            else:  # Linux/Mac
                os.symlink(src, dst_path)
        
        logger.info("\nüéâ Traitement termin√© avec succ√®s!")
        logger.info(f"üìä Donn√©es d'entra√Ænement: {train_data.shape}")
        logger.info(f"üìä Donn√©es de test: {test_data.shape}")
        logger.info(f"üìà Tickers trait√©s: {tickers}")
        
        return normalized_data, train_data, test_data
        
    except Exception as e:
        logger.error(f"Erreur lors du traitement principal: {e}")
        return None, None, None
    


if __name__ == "__main__":
    result = main()
    if result[0] is not None:
        print("‚úÖ Preprocessing r√©ussi!")
    else:
        print("‚ùå Preprocessing √©chou√©!")
        exit(1)