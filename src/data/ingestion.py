"""
Module pour l'ingestion de donn√©es financi√®res depuis Yahoo Finance.
Version corrig√©e compatible avec toutes les versions de yfinance.
"""
import os
import json
import numpy as np
import pandas as pd
import yfinance as yf
from datetime import datetime, timedelta
import logging
from typing import List, Tuple, Optional

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def load_config(config_path: str = "config/default.json") -> dict:
    """
    Charge la configuration depuis un fichier JSON.
    
    Args:
        config_path: Chemin vers le fichier de configuration
        
    Returns:
        dict: Configuration charg√©e
        
    Raises:
        FileNotFoundError: Si le fichier de configuration n'existe pas
        json.JSONDecodeError: Si le fichier JSON est malform√©
    """
    try:
        # G√©rer les chemins relatifs et absolus
        if not os.path.isabs(config_path):
            script_dir = os.path.dirname(os.path.abspath(__file__))
            config_abs_path = os.path.join(script_dir, '..', '..', config_path)
        else:
            config_abs_path = config_path
            
        if not os.path.exists(config_abs_path):
            raise FileNotFoundError(f"Fichier de configuration non trouv√©: {config_abs_path}")
            
        with open(config_abs_path, 'r', encoding='utf-8') as file:
            config = json.load(file)
            
        logger.info(f"Configuration charg√©e depuis: {config_abs_path}")
        return config
        
    except Exception as e:
        logger.error(f"Erreur lors du chargement de la configuration: {e}")
        raise

def validate_tickers(tickers: List[str]) -> List[str]:
    """
    Valide et nettoie la liste des tickers.
    
    Args:
        tickers: Liste des symboles d'actions
        
    Returns:
        List[str]: Liste des tickers valid√©s
    """
    if not tickers or not isinstance(tickers, list):
        raise ValueError("La liste des tickers ne peut pas √™tre vide")
        
    # Nettoyer et valider les tickers
    clean_tickers = []
    for ticker in tickers:
        if isinstance(ticker, str) and ticker.strip():
            clean_tickers.append(ticker.strip().upper())
        else:
            logger.warning(f"Ticker invalide ignor√©: {ticker}")
            
    if not clean_tickers:
        raise ValueError("Aucun ticker valide trouv√©")
        
    logger.info(f"Tickers valid√©s: {clean_tickers}")
    return clean_tickers

def download_stock_data(tickers: List[str], start_date: datetime, end_date: datetime, 
                       retry_count: int = 3) -> pd.DataFrame:
    """
    T√©l√©charge les donn√©es des actions depuis Yahoo Finance avec retry.
    Compatible avec toutes les versions de yfinance.
    
    Args:
        tickers: Liste des symboles d'actions √† t√©l√©charger
        start_date: Date de d√©but
        end_date: Date de fin
        retry_count: Nombre de tentatives en cas d'√©chec
    
    Returns:
        pandas.DataFrame: Donn√©es des actions
        
    Raises:
        Exception: Si le t√©l√©chargement √©choue apr√®s toutes les tentatives
    """
    tickers = validate_tickers(tickers)
    
    for attempt in range(retry_count):
        try:
            logger.info(f"Tentative {attempt + 1}/{retry_count} - T√©l√©chargement des donn√©es pour {len(tickers)} actions...")
            logger.info(f"P√©riode: {start_date.date()} - {end_date.date()}")
            
            # T√©l√©charger les donn√©es avec param√®tres compatibles
            # Utiliser seulement les param√®tres support√©s par toutes les versions
            data = yf.download(
                tickers, 
                start=start_date, 
                end=end_date,
                progress=False,  # D√©sactiver la barre de progression
                threads=True
            )
            
            # V√©rifier si des donn√©es ont √©t√© t√©l√©charg√©es
            if data.empty:
                raise ValueError("Aucune donn√©e t√©l√©charg√©e")
                
            # V√©rifier la qualit√© des donn√©es
            total_cells = data.shape[0] * data.shape[1]
            missing_cells = data.isnull().sum().sum()
            missing_data_ratio = missing_cells / total_cells if total_cells > 0 else 0
            
            if missing_data_ratio > 0.5:
                logger.warning(f"Beaucoup de donn√©es manquantes: {missing_data_ratio:.2%}")
                
            logger.info(f"T√©l√©chargement r√©ussi! Forme des donn√©es: {data.shape}")
            logger.info(f"Donn√©es manquantes: {missing_data_ratio:.2%}")
            
            return data
            
        except Exception as e:
            logger.error(f"Tentative {attempt + 1} √©chou√©e: {e}")
            if attempt == retry_count - 1:
                logger.error("Toutes les tentatives de t√©l√©chargement ont √©chou√©")
                raise
            else:
                logger.info(f"Nouvelle tentative dans 3 secondes...")
                import time
                time.sleep(3)

def check_and_create_dirs(dirs: List[str]) -> None:
    """
    Cr√©e les r√©pertoires s'ils n'existent pas.
    
    Args:
        dirs: Liste des chemins de r√©pertoires √† cr√©er
    """
    for dir_path in dirs:
        try:
            os.makedirs(dir_path, exist_ok=True)
            logger.debug(f"R√©pertoire v√©rifi√©/cr√©√©: {dir_path}")
        except Exception as e:
            logger.error(f"Erreur lors de la cr√©ation du r√©pertoire {dir_path}: {e}")
            raise

def save_data_with_metadata(data: pd.DataFrame, output_path: str, metadata: dict) -> None:
    """
    Sauvegarde les donn√©es avec des m√©tadonn√©es.
    
    Args:
        data: DataFrame √† sauvegarder
        output_path: Chemin de sauvegarde
        metadata: M√©tadonn√©es √† inclure
    """
    try:
        # Sauvegarder les donn√©es
        data.to_csv(output_path)
        logger.info(f"Donn√©es sauvegard√©es dans {output_path}")
        
        # Sauvegarder les m√©tadonn√©es
        metadata_path = output_path.replace('.csv', '_metadata.json')
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, default=str)
        logger.info(f"M√©tadonn√©es sauvegard√©es dans {metadata_path}")
        
    except Exception as e:
        logger.error(f"Erreur lors de la sauvegarde: {e}")
        raise

def main(config_path: str = "config/default.json", 
         tickers: Optional[List[str]] = None,
         start_date: Optional[datetime] = None,
         end_date: Optional[datetime] = None) -> Tuple[pd.DataFrame, List[str], dict]:
    """
    Fonction principale pour l'ingestion des donn√©es.
    
    Args:
        config_path: Chemin du fichier de configuration
        tickers: Liste des tickers (optionnel, sinon lu depuis config)
        start_date: Date de d√©but (optionnel, sinon calcul√©e depuis config)
        end_date: Date de fin (optionnel, sinon datetime.now())
    
    Returns:
        Tuple[pd.DataFrame, List[str], dict]: (donn√©es, tickers, config)
    """
    try:
        # Charger la configuration
        config = load_config(config_path)
        
        # Utiliser les param√®tres fournis ou ceux de la configuration
        if tickers is None:
            tickers = config['data']['tickers']
        if end_date is None:
            end_date = datetime.now()
        if start_date is None:
            lookback_years = config['data']['lookback_years']
            start_date = end_date - timedelta(days=365 * lookback_years)
        
        # Valider les param√®tres
        tickers = validate_tickers(tickers)
        output_dir = config['data']['output_dir']
        
        logger.info(f"D√©marrage de l'ingestion pour {len(tickers)} tickers")
        logger.info(f"P√©riode: {start_date.date()} - {end_date.date()}")
        
        # Cr√©er les r√©pertoires n√©cessaires
        check_and_create_dirs([
            'data/raw',
            output_dir,
            'logs'
        ])
        
        # T√©l√©charger les donn√©es
        stock_data = download_stock_data(tickers, start_date, end_date)
        
        # Pr√©parer les m√©tadonn√©es
        metadata = {
            'tickers': tickers,
            'start_date': start_date.isoformat(),
            'end_date': end_date.isoformat(),
            'download_timestamp': datetime.now().isoformat(),
            'data_shape': list(stock_data.shape),
            'columns': list(stock_data.columns) if hasattr(stock_data.columns, 'tolist') else str(stock_data.columns),
            'missing_data_count': int(stock_data.isnull().sum().sum()),
            'config_used': config_path
        }
        
        # Sauvegarder les donn√©es brutes avec m√©tadonn√©es
        raw_data_path = os.path.join('data/raw', f'stock_data_{end_date.strftime("%Y%m%d")}.csv')
        save_data_with_metadata(stock_data, raw_data_path, metadata)
        
        logger.info("Ingestion des donn√©es termin√©e avec succ√®s")
        return stock_data, tickers, config
        
    except Exception as e:
        logger.error(f"Erreur lors de l'ingestion des donn√©es: {e}")
        raise

if __name__ == "__main__":
    try:
        data, tickers, config = main()
        print(f"‚úÖ Ingestion r√©ussie pour {len(tickers)} tickers")
        print(f"üìä Forme des donn√©es: {data.shape}")
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
        exit(1)