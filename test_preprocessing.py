#!/usr/bin/env python3
"""
Test complet du preprocessing corrig√©.
√Ä placer √† la racine du projet : test_preprocessing.py
"""
import os
import sys
import json
import tempfile
import numpy as np
import pandas as pd
from datetime import datetime, timedelta

# Ajouter le r√©pertoire du projet au PYTHONPATH
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)
sys.path.insert(0, os.path.join(project_root, 'src'))

def create_test_config():
    """Cr√©e un fichier de configuration de test."""
    config = {
        "data": {
            "tickers": ["AAPL", "MSFT", "GOOGL"],
            "lookback_years": 0.1,
            "train_ratio": 0.7,
            "output_dir": "data/processed"
        },
        "environment": {
            "portfolio_value": 10000,
            "window_size": 10,
            "trans_cost": 0.001,
            "return_rate": 0.0001,
            "reward_scaling": 100.0,
            "max_reward_clip": 5.0,
            "min_reward_clip": -5.0
        }
    }
    
    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
        json.dump(config, f, indent=2)
        return f.name

def create_mock_multiindex_data():
    """Cr√©e des donn√©es fictives au format MultiIndex (comme yfinance)."""
    print("üìä Cr√©ation de donn√©es fictives MultiIndex...")
    
    dates = pd.date_range(start='2024-01-01', periods=100, freq='D')
    tickers = ['AAPL', 'MSFT', 'GOOGL']
    features = ['Open', 'High', 'Low', 'Close', 'Volume']
    
    # Cr√©er MultiIndex
    columns = pd.MultiIndex.from_product([features, tickers])
    data = pd.DataFrame(index=dates, columns=columns)
    
    # Remplir avec des donn√©es r√©alistes
    for ticker in tickers:
        base_price = 100 + hash(ticker) % 100  # Prix de base diff√©rent
        
        # G√©n√©rer une s√©rie de prix
        returns = np.random.normal(0.001, 0.02, len(dates))
        prices = base_price * np.exp(np.cumsum(returns))
        
        for i, price in enumerate(prices):
            # OHLC r√©aliste
            volatility = 0.01
            open_price = price * (1 + np.random.normal(0, volatility/2))
            high = price * (1 + np.random.uniform(0, volatility))
            low = price * (1 - np.random.uniform(0, volatility))
            close = price
            volume = np.random.randint(1000000, 10000000)
            
            # S'assurer que High >= max(Open, Close) et Low <= min(Open, Close)
            high = max(high, open_price, close)
            low = min(low, open_price, close)
            
            data.loc[dates[i], ('Open', ticker)] = open_price
            data.loc[dates[i], ('High', ticker)] = high
            data.loc[dates[i], ('Low', ticker)] = low
            data.loc[dates[i], ('Close', ticker)] = close
            data.loc[dates[i], ('Volume', ticker)] = volume
    
    print(f"‚úÖ Donn√©es MultiIndex cr√©√©es: {data.shape}")
    return data

def create_mock_long_format_data():
    """Cr√©e des donn√©es fictives au format long avec colonne ticker."""
    print("üìä Cr√©ation de donn√©es fictives format long...")
    
    dates = pd.date_range(start='2024-01-01', periods=50, freq='D')
    tickers = ['AAPL', 'MSFT']
    
    data_list = []
    for ticker in tickers:
        base_price = 100 + hash(ticker) % 50
        returns = np.random.normal(0.001, 0.02, len(dates))
        prices = base_price * np.exp(np.cumsum(returns))
        
        for i, (date, price) in enumerate(zip(dates, prices)):
            volatility = 0.01
            data_list.append({
                'ticker': ticker,
                'Open': price * (1 + np.random.normal(0, volatility/2)),
                'High': price * (1 + np.random.uniform(0, volatility)),
                'Low': price * (1 - np.random.uniform(0, volatility)),
                'Close': price,
                'Volume': np.random.randint(1000000, 5000000)
            })
    
    data = pd.DataFrame(data_list)
    data = data.set_index(pd.date_range(start='2024-01-01', periods=len(data), freq='D'))
    
    print(f"‚úÖ Donn√©es format long cr√©√©es: {data.shape}")
    return data

def test_format_detection():
    """Test de d√©tection de format des donn√©es."""
    print("üîç Test de d√©tection de format...")
    
    try:
        from src.data.preprocessing import detect_data_format
        
        # Test 1: Format MultiIndex
        print("   Test 1: Format MultiIndex...")
        multiindex_data = create_mock_multiindex_data()
        format_info = detect_data_format(multiindex_data)
        
        assert format_info['is_multiindex'] == True
        assert len(format_info['tickers']) == 3
        assert 'Close' in format_info['detected_features']
        print("   ‚úÖ D√©tection MultiIndex r√©ussie")
        
        # Test 2: Format long
        print("   Test 2: Format long...")
        long_data = create_mock_long_format_data()
        format_info = detect_data_format(long_data)
        
        assert format_info['has_ticker_column'] == True
        assert 'Close' in format_info['features']
        print("   ‚úÖ D√©tection format long r√©ussie")
        
        print("‚úÖ Tests de d√©tection de format r√©ussis")
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur d√©tection format: {e}")
        import traceback
        traceback.print_exc()
        return False



def create_better_csv_data():
    """Cr√©e des donn√©es CSV qui pr√©servent la structure MultiIndex."""
    print("üìä Cr√©ation de donn√©es CSV avec structure pr√©serv√©e...")
    
    dates = pd.date_range(start='2024-01-01', periods=50, freq='D')
    tickers = ['AAPL', 'MSFT', 'GOOGL']
    features = ['Open', 'High', 'Low', 'Close', 'Volume']
    
    # Cr√©er un dict pour construire le DataFrame
    data_dict = {}
    
    for ticker in tickers:
        base_price = 100 + hash(ticker) % 50
        returns = np.random.normal(0.001, 0.02, len(dates))
        prices = base_price * np.exp(np.cumsum(returns))
        
        for i, price in enumerate(prices):
            volatility = 0.01
            open_price = price * (1 + np.random.normal(0, volatility/2))
            high = price * (1 + np.random.uniform(0, volatility))
            low = price * (1 - np.random.uniform(0, volatility))
            close = price
            volume = np.random.randint(1000000, 10000000)
            
            high = max(high, open_price, close)
            low = min(low, open_price, close)
            
            # Ajouter au dictionnaire avec noms de colonnes simples
            data_dict[f'{ticker}_Open'] = data_dict.get(f'{ticker}_Open', []) + [open_price]
            data_dict[f'{ticker}_High'] = data_dict.get(f'{ticker}_High', []) + [high]
            data_dict[f'{ticker}_Low'] = data_dict.get(f'{ticker}_Low', []) + [low]
            data_dict[f'{ticker}_Close'] = data_dict.get(f'{ticker}_Close', []) + [close]
            data_dict[f'{ticker}_Volume'] = data_dict.get(f'{ticker}_Volume', []) + [volume]
    
    # Cr√©er DataFrame simple (sera plus facile √† traiter)
    data = pd.DataFrame(data_dict, index=dates)
    
    print(f"‚úÖ Donn√©es CSV cr√©√©es: {data.shape}")
    return data

def test_data_preprocessing():
    """Test de pr√©traitement des donn√©es."""
    print("üîÑ Test de pr√©traitement des donn√©es...")
    
    try:
        from src.data.preprocessing import preprocess_data
        
        # Test avec donn√©es MultiIndex
        print("   Test 1: Pr√©traitement MultiIndex...")
        multiindex_data = create_mock_multiindex_data()
        tickers = ['AAPL', 'MSFT', 'GOOGL']
        
        result = preprocess_data(multiindex_data, tickers)
        
        # V√©rifications
        assert result.shape == (5, 3, 100), f"Forme incorrecte: {result.shape}"
        assert result.dtype == np.float32, f"Type incorrect: {result.dtype}"
        assert not np.isnan(result).any(), "Des NaN d√©tect√©s"
        assert not np.isinf(result).any(), "Des inf d√©tect√©s"
        
        print(f"      ‚Üí Forme: {result.shape}")
        print(f"      ‚Üí Type: {result.dtype}")
        print(f"      ‚Üí Valeurs moyennes: {np.mean(result, axis=(1,2))}")
        print("   ‚úÖ Pr√©traitement MultiIndex r√©ussi")
        
        # Test avec donn√©es format long - CORRECTION ICI
        print("   Test 2: Pr√©traitement format long...")
        long_data = create_mock_long_format_data()
        tickers = ['AAPL', 'MSFT']
        
        result = preprocess_data(long_data, tickers)
        
        # Correction: les donn√©es long ont 100 p√©riodes, pas 50
        expected_periods = 100  # Chang√© de 50 √† 100
        assert result.shape == (5, 2, expected_periods), f"Forme incorrecte: {result.shape}"
        assert not np.isnan(result).any(), "Des NaN d√©tect√©s"
        
        print(f"      ‚Üí Forme: {result.shape}")
        print("   ‚úÖ Pr√©traitement format long r√©ussi")
        
        print("‚úÖ Tests de pr√©traitement r√©ussis")
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur pr√©traitement: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_data_normalization():
    """Test de normalisation des donn√©es."""
    print("üéØ Test de normalisation...")
    
    try:
        from src.data.preprocessing import preprocess_data, normalize_data
        
        # Cr√©er des donn√©es de test
        multiindex_data = create_mock_multiindex_data()
        tickers = ['AAPL', 'MSFT']
        
        # Pr√©traiter
        processed_data = preprocess_data(multiindex_data, tickers)
        
        # Normaliser
        normalized_data = normalize_data(processed_data)
        
        # V√©rifications
        assert normalized_data.shape == processed_data.shape
        assert normalized_data.dtype == np.float32
        
        # V√©rifier que les prix sont normalis√©s (premier point = 1)
        for stock_idx in range(normalized_data.shape[1]):
            for feature_idx in range(4):  # OHLC
                first_price = normalized_data[feature_idx, stock_idx, 0]
                if first_price > 0:
                    assert abs(first_price - 1.0) < 0.1, f"Normalisation incorrecte: {first_price}"
        
        # V√©rifier que les volumes sont entre 0 et 1
        if normalized_data.shape[0] >= 5:
            volume_data = normalized_data[4, :, :]
            assert np.all(volume_data >= 0), "Volumes n√©gatifs d√©tect√©s"
            assert np.all(volume_data <= 1.1), "Volumes > 1 d√©tect√©s"  # Petite tol√©rance
        
        print(f"   ‚Üí Forme: {normalized_data.shape}")
        print(f"   ‚Üí Prix moyens normalis√©s: {np.mean(normalized_data[:4], axis=(1,2))}")
        if normalized_data.shape[0] >= 5:
            print(f"   ‚Üí Volume moyen normalis√©: {np.mean(normalized_data[4]):.3f}")
        
        print("‚úÖ Test de normalisation r√©ussi")
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur normalisation: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_data_splitting():
    """Test de division des donn√©es."""
    print("‚úÇÔ∏è Test de division des donn√©es...")
    
    try:
        from src.data.preprocessing import preprocess_data, normalize_data, split_data
        
        # Cr√©er et traiter des donn√©es
        multiindex_data = create_mock_multiindex_data()
        tickers = ['AAPL', 'MSFT']
        
        processed_data = preprocess_data(multiindex_data, tickers)
        normalized_data = normalize_data(processed_data)
        
        # Tester diff√©rents ratios
        ratios = [0.6, 0.7, 0.8]
        
        for ratio in ratios:
            train_data, test_data = split_data(normalized_data, ratio)
            
            total_periods = normalized_data.shape[2]
            expected_train = int(total_periods * ratio)
            
            assert train_data.shape[2] + test_data.shape[2] == total_periods
            assert abs(train_data.shape[2] - expected_train) <= 1  # Tol√©rance de 1
            
            print(f"   ‚Üí Ratio {ratio}: Train={train_data.shape[2]}, Test={test_data.shape[2]}")
        
        print("‚úÖ Test de division r√©ussi")
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur division: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_environment_compatibility():
    """Test de compatibilit√© avec l'environnement RL."""
    print("ü§ñ Test de compatibilit√© avec l'environnement RL...")
    
    try:
        from src.data.preprocessing import main as preprocess_main
        from src.environment.portfolio_env import PortfolioEnv
        
        # Cr√©er des donn√©es simples pour √©viter le probl√®me MultiIndex
        print("   Cr√©ation de donn√©es compatibles CSV...")
        simple_data = create_better_csv_data()
        temp_data_path = "temp_test_data.csv"
        simple_data.to_csv(temp_data_path)
        
        # Configuration de test
        config_path = create_test_config()
        
        try:
            # Ex√©cuter le preprocessing
            normalized_data, train_data, test_data = preprocess_main(
                config_path=config_path,
                use_existing_data=True,
                data_path=temp_data_path
            )
            
            if normalized_data is None:
                print("   ‚ö†Ô∏è Preprocessing a √©chou√©, cr√©ation de donn√©es de test...")
                # Cr√©er des donn√©es de test directement
                normalized_data = np.random.rand(5, 3, 50).astype(np.float32)
                train_data = normalized_data[:, :, :35]
                test_data = normalized_data[:, :, 35:]
            
            # Tester avec l'environnement RL
            print("   Test avec environnement RL...")
            env = PortfolioEnv(data=train_data, window_size=10)
            
            # Test d'un √©pisode court
            obs, info = env.reset()
            action = np.array([0.4, 0.3, 0.3])
            next_obs, reward, terminated, truncated, step_info = env.step(action)
            
            # V√©rifications
            assert isinstance(reward, (int, float, np.number))
            assert 'portfolio_value' in step_info
            
            print(f"   ‚Üí Donn√©es compatibles avec environnement RL")
            print(f"   ‚Üí Test step: reward={reward:.3f}, portfolio=${step_info['portfolio_value']:.2f}")
            
            print("‚úÖ Test de compatibilit√© r√©ussi")
            return True
            
        finally:
            # Nettoyer
            if os.path.exists(temp_data_path):
                os.remove(temp_data_path)
            if os.path.exists(config_path):
                os.remove(config_path)
        
    except Exception as e:
        print(f"‚ùå Erreur compatibilit√©: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_complete_pipeline():
    """Test du pipeline complet."""
    print("üîÑ Test du pipeline complet...")
    
    try:
        from src.data.preprocessing import main as preprocess_main
        
        # Configuration
        config_path = create_test_config()
        
        # Cr√©er des donn√©es d'entr√©e simples (pas MultiIndex)
        temp_data_path = "temp_pipeline_test.csv"
        simple_data = create_better_csv_data()
        simple_data.to_csv(temp_data_path)
        
        try:
            print("   Ex√©cution du pipeline complet...")
            normalized_data, train_data, test_data = preprocess_main(
                config_path=config_path,
                use_existing_data=True,
                data_path=temp_data_path
            )
            
            if normalized_data is not None:
                # V√©rifications finales
                assert train_data is not None
                assert test_data is not None
                assert normalized_data.ndim == 3
                assert train_data.ndim == 3
                assert test_data.ndim == 3
                
                print(f"   ‚Üí Donn√©es normalis√©es: {normalized_data.shape}")
                print(f"   ‚Üí Donn√©es d'entra√Ænement: {train_data.shape}")
                print(f"   ‚Üí Donn√©es de test: {test_data.shape}")
                
                # V√©rifier que les fichiers sont cr√©√©s
                output_dir = "data/processed"
                expected_files = [
                    "stock_data_normalized_latest.npy",
                    "stock_data_train_latest.npy", 
                    "stock_data_test_latest.npy"
                ]
                
                for filename in expected_files:
                    filepath = os.path.join(output_dir, filename)
                    if os.path.exists(filepath):
                        print(f"   ‚úÖ Fichier cr√©√©: {filename}")
                    else:
                        print(f"   ‚ö†Ô∏è Fichier manquant: {filename}")
                
                print("‚úÖ Pipeline complet r√©ussi")
                return True
            else:
                print("‚ö†Ô∏è Pipeline a retourn√© None, mais pas d'erreur")
                return False
                
        finally:
            # Nettoyer
            if os.path.exists(temp_data_path):
                os.remove(temp_data_path)
            if os.path.exists(config_path):
                os.remove(config_path)
        
    except Exception as e:
        print(f"‚ùå Erreur pipeline: {e}")
        import traceback
        traceback.print_exc()
        return False
    

def main():
    """Fonction principale de test."""
    print("üöÄ Test complet du preprocessing corrig√©")
    print("=" * 60)
    
    # Cr√©er les r√©pertoires n√©cessaires
    os.makedirs("data/processed", exist_ok=True)
    os.makedirs("src/data", exist_ok=True)
    
    # Tests s√©quentiels
    tests = [
        ("D√©tection de format", test_format_detection),
        ("Pr√©traitement des donn√©es", test_data_preprocessing),
        ("Normalisation des donn√©es", test_data_normalization),
        ("Division des donn√©es", test_data_splitting),
        ("Compatibilit√© environnement RL", test_environment_compatibility),
        ("Pipeline complet", test_complete_pipeline),
    ]
    
    results = []
    
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        
        try:
            result = test_func()
            results.append(result if result is not None else False)
        except Exception as e:
            print(f"‚ùå Test {test_name} √©chou√©: {e}")
            results.append(False)
    
    # R√©sum√© final
    print("\n" + "=" * 60)
    print("üìã R√âSUM√â DES TESTS")
    print("=" * 60)
    
    passed = sum(1 for r in results if r)
    total = len(results)
    
    for i, (test_name, _) in enumerate(tests):
        status = "‚úÖ PASS√â" if results[i] else "‚ùå √âCHOU√â"
        print(f"   {status}: {test_name}")
    
    print(f"\nüìä Score: {passed}/{total} tests pass√©s ({passed/total*100:.1f}%)")
    
    if passed == total:
        print("\nüéâ TOUS LES TESTS SONT PASS√âS!")
        print("\nüìã Le preprocessing est maintenant pr√™t!")
        print("üéØ √âTAPE 3 VALID√âE!")
        print("\nüìã Prochaines √©tapes:")
        print("   4. ü§ñ Test du pipeline d'entra√Ænement complet")
        print("   5. üîÑ Int√©gration MLflow et monitoring")
        print("   6. üöÄ D√©ploiement et API")
    else:
        print(f"\n‚ö†Ô∏è  {total-passed} test(s) ont √©chou√©.")
        print("V√©rifiez les erreurs ci-dessus avant de continuer.")
        return False
    
    return True

if __name__ == "__main__":
    success = main()
    if not success:
        sys.exit(1)