#!/usr/bin/env python3
"""
Test complet du pipeline d'entra√Ænement avec MLflow.
√Ä placer √† la racine du projet : test_training_pipeline.py
"""
import os
import sys
import json
import tempfile
import subprocess
import signal
import time
from datetime import datetime

# Ajouter le r√©pertoire du projet au PYTHONPATH
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)
sys.path.insert(0, os.path.join(project_root, 'src'))

def create_test_config():
    """Cr√©e un fichier de configuration de test."""
    config = {
        "data": {
            "tickers": ["AAPL", "MSFT", "GOOGL"],
            "lookback_years": 0.1,
            "train_ratio": 0.7,
            "output_dir": "data/processed"
        },
        "environment": {
            "portfolio_value": 10000,
            "window_size": 10,
            "trans_cost": 0.001,
            "return_rate": 0.0001,
            "reward_scaling": 100.0,
            "max_reward_clip": 5.0,
            "min_reward_clip": -5.0,
            "normalize_observations": True,
            "risk_penalty": 0.1
        },
        "training": {
            "algorithm": "PPO",
            "learning_rate": 0.001,
            "n_steps": 512,
            "batch_size": 32,
            "ent_coef": 0.01,
            "clip_range": 0.2,
            "max_grad_norm": 0.5,
            "gae_lambda": 0.95,
            "gamma": 0.99,
            "n_epochs": 5,
            "total_timesteps": 5000,  # Petit nombre pour test rapide
            "log_dir": "logs/training"
        }
    }
    
    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
        json.dump(config, f, indent=2)
        return f.name

def start_mlflow_server(port=5000):
    """D√©marre un serveur MLflow local pour les tests."""
    try:
        # V√©rifier si MLflow est d√©j√† en cours
        import requests
        response = requests.get(f"http://localhost:{port}", timeout=2)
        if response.status_code == 200:
            print(f"‚úÖ MLflow d√©j√† en cours sur le port {port}")
            return None
    except:
        pass
    
    # D√©marrer MLflow
    print(f"üöÄ D√©marrage de MLflow sur le port {port}...")
    
    mlflow_cmd = [
        sys.executable, "-m", "mlflow", "server",
        "--host", "0.0.0.0",
        "--port", str(port),
        "--backend-store-uri", "sqlite:///test_mlflow.db",
        "--default-artifact-root", "./test_artifacts"
    ]
    
    try:
        process = subprocess.Popen(
            mlflow_cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            env={**os.environ, "MLFLOW_TRACKING_URI": f"http://localhost:{port}"}
        )
        
        # Attendre que MLflow soit pr√™t
        max_wait = 30
        for i in range(max_wait):
            try:
                import requests
                response = requests.get(f"http://localhost:{port}", timeout=1)
                if response.status_code == 200:
                    print(f"‚úÖ MLflow d√©marr√© avec succ√®s!")
                    return process
            except:
                pass
            time.sleep(1)
            print(f"‚è≥ Attente MLflow... ({i+1}/{max_wait})")
        
        print("‚ùå Timeout: MLflow n'a pas d√©marr√©")
        process.terminate()
        return None
        
    except Exception as e:
        print(f"‚ùå Erreur lors du d√©marrage de MLflow: {e}")
        return None

def create_test_data():
    """Cr√©e des donn√©es de test pour l'entra√Ænement."""
    print("üìä Cr√©ation de donn√©es de test...")
    
    import numpy as np
    from src.data.preprocessing import save_data
    
    # Cr√©er des donn√©es factices
    n_features, n_stocks, n_time_periods = 5, 3, 100
    data = np.random.rand(n_features, n_stocks, n_time_periods).astype(np.float32)
    
    # Simuler des prix r√©alistes (OHLC)
    for stock in range(n_stocks):
        base_price = 100 + stock * 50
        for t in range(n_time_periods):
            close = base_price * (1 + np.random.normal(0, 0.02))
            data[0, stock, t] = close * (1 + np.random.normal(0, 0.01))  # Open
            data[1, stock, t] = close * (1 + abs(np.random.normal(0, 0.01)))  # High
            data[2, stock, t] = close * (1 - abs(np.random.normal(0, 0.01)))  # Low
            data[3, stock, t] = close  # Close
            data[4, stock, t] = np.random.randint(100000, 1000000)  # Volume
    
    # Division train/test
    split_point = int(n_time_periods * 0.7)
    train_data = data[:, :, :split_point]
    test_data = data[:, :, split_point:]
    
    # Sauvegarder
    os.makedirs("data/processed", exist_ok=True)
    
    save_data(data, "data/processed/stock_data_normalized_latest.npy")
    save_data(train_data, "data/processed/stock_data_train_latest.npy")
    save_data(test_data, "data/processed/stock_data_test_latest.npy")
    
    print(f"‚úÖ Donn√©es cr√©√©es: train={train_data.shape}, test={test_data.shape}")
    return True

def test_training_imports():
    """Test des imports n√©cessaires."""
    print("üì¶ Test des imports...")
    
    try:
        import mlflow
        print(f"   ‚úÖ MLflow: {mlflow.__version__}")
        
        from stable_baselines3 import PPO
        print(f"   ‚úÖ Stable-Baselines3: {PPO}")
        
        from src.models.train import train_portfolio_agent, load_training_data
        print("   ‚úÖ Module d'entra√Ænement local")
        
        from src.environment.portfolio_env import PortfolioEnv
        print("   ‚úÖ Environnement RL")
        
        return True
        
    except ImportError as e:
        print(f"   ‚ùå Import √©chou√©: {e}")
        return False

def test_data_loading():
    """Test du chargement des donn√©es."""
    print("üìä Test du chargement des donn√©es...")
    
    try:
        from src.models.train import load_training_data
        
        config_path = create_test_config()
        train_data, test_data = load_training_data(config_path)
        
        if train_data is not None and test_data is not None:
            print(f"   ‚úÖ Train: {train_data.shape}")
            print(f"   ‚úÖ Test: {test_data.shape}")
            os.unlink(config_path)
            return True
        else:
            print("   ‚ùå Donn√©es non charg√©es")
            os.unlink(config_path)
            return False
            
    except Exception as e:
        print(f"   ‚ùå Erreur: {e}")
        return False

def test_environment_creation():
    """Test de cr√©ation de l'environnement."""
    print("üèóÔ∏è Test de cr√©ation de l'environnement...")
    
    try:
        from src.environment.portfolio_env import create_env_from_config
        import numpy as np
        
        # Donn√©es factices
        test_data = np.random.rand(5, 3, 50).astype(np.float32)
        config_path = create_test_config()
        
        env = create_env_from_config(test_data, config_path)
        
        # Test basique
        obs, info = env.reset()
        action = env.action_space.sample()
        next_obs, reward, terminated, truncated, step_info = env.step(action)
        
        print("   ‚úÖ Environnement cr√©√© et test√©")
        os.unlink(config_path)
        return True
        
    except Exception as e:
        print(f"   ‚ùå Erreur: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_mlflow_connection():
    """Test de connexion √† MLflow."""
    print("üîó Test de connexion MLflow...")
    
    try:
        import mlflow
        
        # Configurer l'URI
        mlflow_uri = os.environ.get("MLFLOW_TRACKING_URI", "http://localhost:5000")
        mlflow.set_tracking_uri(mlflow_uri)
        
        # Test de connexion
        client = mlflow.tracking.MlflowClient()
        experiments = client.search_experiments()
        
        print(f"   ‚úÖ Connexion MLflow r√©ussie ({len(experiments)} exp√©riences)")
        return True
        
    except Exception as e:
        print(f"   ‚ùå Erreur MLflow: {e}")
        return False

def test_short_training():
    """Test d'un entra√Ænement court."""
    print("üéØ Test d'entra√Ænement court...")
    
    try:
        from src.models.train import train_portfolio_agent
        
        config_path = create_test_config()
        
        # Entra√Ænement rapide
        print("   ‚è≥ D√©marrage de l'entra√Ænement (5000 steps)...")
        model, training_info = train_portfolio_agent(
            config_path=config_path,
            run_name="test_training"
        )
        
        if model is not None and training_info is not None:
            print(f"   ‚úÖ Entra√Ænement r√©ussi!")
            print(f"      üìç Run ID: {training_info['run_id']}")
            print(f"      üèÜ Meilleure r√©compense: {training_info['best_reward']:.4f}")
            print(f"      ‚è±Ô∏è Temps: {training_info['training_time']:.1f}s")
            
            # V√©rifier les fichiers cr√©√©s
            final_model = training_info['final_model_path'] + ".zip"
            if os.path.exists(final_model):
                print(f"      ‚úÖ Mod√®le sauvegard√©: {final_model}")
            else:
                print(f"      ‚ö†Ô∏è Mod√®le non trouv√©: {final_model}")
            
            os.unlink(config_path)
            return True, training_info
        else:
            print("   ‚ùå Entra√Ænement √©chou√©")
            os.unlink(config_path)
            return False, None
            
    except Exception as e:
        print(f"   ‚ùå Erreur: {e}")
        import traceback
        traceback.print_exc()
        return False, None

def test_model_evaluation():
    """Test d'√©valuation du mod√®le."""
    print("üìä Test d'√©valuation du mod√®le...")
    
    try:
        from src.models.train import evaluate_trained_model
        
        # Utiliser le dernier mod√®le entra√Æn√©
        config_path = create_test_config()
        
        # Chercher un mod√®le r√©cent
        logs_dir = "logs/training"
        if os.path.exists(logs_dir):
            training_dirs = [d for d in os.listdir(logs_dir) if d.startswith("training_")]
            if training_dirs:
                latest_dir = max(training_dirs)
                model_path = os.path.join(logs_dir, latest_dir, "final_model")
                
                if os.path.exists(model_path + ".zip"):
                    metrics = evaluate_trained_model(model_path, config_path)
                    
                    if metrics:
                        print("   ‚úÖ √âvaluation r√©ussie:")
                        for key, value in metrics.items():
                            print(f"      {key}: {value:.4f}")
                        os.unlink(config_path)
                        return True
        
        print("   ‚ö†Ô∏è Aucun mod√®le trouv√© pour l'√©valuation")
        os.unlink(config_path)
        return False
        
    except Exception as e:
        print(f"   ‚ùå Erreur: {e}")
        return False

def test_mlflow_ui_access():
    """Test d'acc√®s √† l'UI MLflow."""
    print("üåê Test d'acc√®s √† l'UI MLflow...")
    
    try:
        import requests
        
        mlflow_url = "http://localhost:5000"
        response = requests.get(mlflow_url, timeout=5)
        
        if response.status_code == 200:
            print(f"   ‚úÖ UI MLflow accessible: {mlflow_url}")
            print(f"   üåê Ouvrez votre navigateur sur: {mlflow_url}")
            return True
        else:
            print(f"   ‚ùå UI MLflow non accessible (status: {response.status_code})")
            return False
            
    except Exception as e:
        print(f"   ‚ùå Erreur: {e}")
        return False

def cleanup_test_files():
    """Nettoie les fichiers de test."""
    print("üßπ Nettoyage des fichiers de test...")
    
    files_to_clean = [
        "test_mlflow.db",
        "test_artifacts",
        "data/processed/stock_data_*_latest.npy"
    ]
    
    for pattern in files_to_clean:
        if "*" in pattern:
            import glob
            for file in glob.glob(pattern):
                try:
                    os.remove(file)
                    print(f"   üóëÔ∏è Supprim√©: {file}")
                except:
                    pass
        else:
            if os.path.exists(pattern):
                try:
                    if os.path.isdir(pattern):
                        import shutil
                        shutil.rmtree(pattern)
                    else:
                        os.remove(pattern)
                    print(f"   üóëÔ∏è Supprim√©: {pattern}")
                except:
                    pass

def main():
    """Fonction principale de test."""
    print("üöÄ Test complet du pipeline d'entra√Ænement MLflow")
    print("=" * 60)
    
    # Variables de contr√¥le
    mlflow_process = None
    cleanup_on_exit = True
    
    try:
        # Cr√©er les r√©pertoires n√©cessaires
        os.makedirs("data/processed", exist_ok=True)
        os.makedirs("logs/training", exist_ok=True)
        os.makedirs("artifacts", exist_ok=True)
        
        # Tests s√©quentiels
        tests = [
            ("Imports n√©cessaires", test_training_imports),
            ("Cr√©ation de donn√©es test", create_test_data),
            ("D√©marrage MLflow", lambda: start_mlflow_server()),
            ("Connexion MLflow", test_mlflow_connection),
            ("Chargement des donn√©es", test_data_loading),
            ("Cr√©ation environnement", test_environment_creation),
            ("Entra√Ænement court", test_short_training),
            ("√âvaluation mod√®le", test_model_evaluation),
            ("Acc√®s UI MLflow", test_mlflow_ui_access),
        ]
        
        results = []
        training_info = None
        
        for i, (test_name, test_func) in enumerate(tests):
            print(f"\n{'='*20} {i+1}. {test_name} {'='*20}")
            
            try:
                if test_name == "D√©marrage MLflow":
                    mlflow_process = test_func()
                    result = mlflow_process is not None
                    # Configurer la variable d'environnement
                    os.environ["MLFLOW_TRACKING_URI"] = "http://localhost:5000"
                elif test_name == "Entra√Ænement court":
                    result, training_info = test_func()
                else:
                    result = test_func()
                
                results.append(result if result is not None else False)
                
                if not result and test_name in ["Imports n√©cessaires", "Cr√©ation de donn√©es test"]:
                    print(f"‚ùå Test critique √©chou√©: {test_name}")
                    break
                    
            except Exception as e:
                print(f"‚ùå Test {test_name} √©chou√©: {e}")
                results.append(False)
        
        # R√©sum√© des r√©sultats
        print("\n" + "=" * 60)
        print("üìã R√âSUM√â DES TESTS")
        print("=" * 60)
        
        passed = sum(1 for r in results if r)
        total = len(results)
        
        for i, (test_name, _) in enumerate(tests[:len(results)]):
            status = "‚úÖ PASS√â" if results[i] else "‚ùå √âCHOU√â"
            print(f"   {status}: {test_name}")
        
        print(f"\nüìä Score: {passed}/{total} tests pass√©s ({passed/total*100:.1f}%)")
        
        if passed >= total - 1:  # Accepter 1 √©chec sur les tests non critiques
            print("\nüéâ PIPELINE D'ENTRA√éNEMENT FONCTIONNEL!")
            print("\nüìã Ce qui fonctionne:")
            print("   ‚úÖ Entra√Ænement avec MLflow")
            print("   ‚úÖ Tracking des m√©triques")
            print("   ‚úÖ Sauvegarde des mod√®les")
            print("   ‚úÖ √âvaluation automatique")
            print("   ‚úÖ Interface MLflow")
            
            if training_info:
                print(f"\nüéØ DERNI√àRE SESSION D'ENTRA√éNEMENT:")
                print(f"   üìç Run ID: {training_info.get('run_id', 'N/A')}")
                print(f"   üèÜ Meilleure r√©compense: {training_info.get('best_reward', 'N/A')}")
                print(f"   ‚è±Ô∏è Temps d'entra√Ænement: {training_info.get('training_time', 'N/A')}s")
            
            print(f"\nüåê MLflow UI: http://localhost:5000")
            print("\nüìã √âTAPE 4 VALID√âE!")
            print("üéØ Prochaine √©tape: API de pr√©diction (√âtape 5)")
            
            # Demander si on veut garder MLflow ouvert
            try:
                response = input("\n‚ùì Garder MLflow ouvert pour exploration ? (y/N): ")
                if response.lower() in ['y', 'yes']:
                    cleanup_on_exit = False
                    print("üí° MLflow reste ouvert. Fermez manuellement quand termin√©.")
            except KeyboardInterrupt:
                print("\n‚èπÔ∏è Interruption utilisateur")
        
        else:
            print(f"\n‚ö†Ô∏è {total-passed} test(s) ont √©chou√©.")
            print("V√©rifiez les erreurs ci-dessus avant de continuer.")
            return False
    
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è Tests interrompus par l'utilisateur")
        return False
    
    finally:
        # Nettoyage
        if cleanup_on_exit:
            if mlflow_process:
                print("\nüõë Arr√™t de MLflow...")
                mlflow_process.terminate()
                try:
                    mlflow_process.wait(timeout=5)
                except:
                    mlflow_process.kill()
            
            cleanup_test_files()
        else:
            print("\nüí° MLflow toujours en cours. Pour l'arr√™ter:")
            print(f"   kill {mlflow_process.pid}")
    
    return True

if __name__ == "__main__":
    success = main()
    if not success:
        sys.exit(1)