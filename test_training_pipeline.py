#!/usr/bin/env python3
"""
Test complet du pipeline d'entraÃ®nement avec MLflow.
Ã€ placer Ã  la racine du projet : test_training_pipeline.py
"""
import os
import sys
import json
import tempfile
import subprocess
import signal
import time
from datetime import datetime

# Ajouter le rÃ©pertoire du projet au PYTHONPATH
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)
sys.path.insert(0, os.path.join(project_root, 'src'))

def create_test_config():
    """CrÃ©e un fichier de configuration de test."""
    config = {
        "data": {
            "tickers": ["AAPL", "MSFT", "GOOGL"],
            "lookback_years": 0.1,
            "train_ratio": 0.7,
            "output_dir": "data/processed"
        },
        "environment": {
            "portfolio_value": 10000,
            "window_size": 10,
            "trans_cost": 0.001,
            "return_rate": 0.0001,
            "reward_scaling": 100.0,
            "max_reward_clip": 5.0,
            "min_reward_clip": -5.0,
            "normalize_observations": True,
            "risk_penalty": 0.1
        },
        "training": {
            "algorithm": "PPO",
            "learning_rate": 0.001,
            "n_steps": 512,
            "batch_size": 32,
            "ent_coef": 0.01,
            "clip_range": 0.2,
            "max_grad_norm": 0.5,
            "gae_lambda": 0.95,
            "gamma": 0.99,
            "n_epochs": 5,
            "total_timesteps": 5000,  # Petit nombre pour test rapide
            "log_dir": "logs/training"
        }
    }
    
    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
        json.dump(config, f, indent=2)
        return f.name

def start_mlflow_server(port=5000):
    """DÃ©marre un serveur MLflow local pour les tests."""
    try:
        # VÃ©rifier si MLflow est dÃ©jÃ  en cours
        import requests
        response = requests.get(f"http://localhost:{port}", timeout=2)
        if response.status_code == 200:
            print(f"âœ… MLflow dÃ©jÃ  en cours sur le port {port}")
            return None
    except:
        pass
    
    # DÃ©marrer MLflow
    print(f"ğŸš€ DÃ©marrage de MLflow sur le port {port}...")
    
    mlflow_cmd = [
        sys.executable, "-m", "mlflow", "server",
        "--host", "0.0.0.0",
        "--port", str(port),
        "--backend-store-uri", "sqlite:///test_mlflow.db",
        "--default-artifact-root", "./test_artifacts"
    ]
    
    try:
        process = subprocess.Popen(
            mlflow_cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            env={**os.environ, "MLFLOW_TRACKING_URI": f"http://localhost:{port}"}
        )
        
        # Attendre que MLflow soit prÃªt
        max_wait = 30
        for i in range(max_wait):
            try:
                import requests
                response = requests.get(f"http://localhost:{port}", timeout=1)
                if response.status_code == 200:
                    print(f"âœ… MLflow dÃ©marrÃ© avec succÃ¨s!")
                    return process
            except:
                pass
            time.sleep(1)
            print(f"â³ Attente MLflow... ({i+1}/{max_wait})")
        
        print("âŒ Timeout: MLflow n'a pas dÃ©marrÃ©")
        process.terminate()
        return None
        
    except Exception as e:
        print(f"âŒ Erreur lors du dÃ©marrage de MLflow: {e}")
        return None

def create_test_data():
    """CrÃ©e des donnÃ©es de test pour l'entraÃ®nement."""
    print("ğŸ“Š CrÃ©ation de donnÃ©es de test...")
    
    import numpy as np
    from src.data.preprocessing import save_data
    
    # CrÃ©er des donnÃ©es factices
    n_features, n_stocks, n_time_periods = 5, 3, 100
    data = np.random.rand(n_features, n_stocks, n_time_periods).astype(np.float32)
    
    # Simuler des prix rÃ©alistes (OHLC)
    for stock in range(n_stocks):
        base_price = 100 + stock * 50
        for t in range(n_time_periods):
            close = base_price * (1 + np.random.normal(0, 0.02))
            data[0, stock, t] = close * (1 + np.random.normal(0, 0.01))  # Open
            data[1, stock, t] = close * (1 + abs(np.random.normal(0, 0.01)))  # High
            data[2, stock, t] = close * (1 - abs(np.random.normal(0, 0.01)))  # Low
            data[3, stock, t] = close  # Close
            data[4, stock, t] = np.random.randint(100000, 1000000)  # Volume
    
    # Division train/test
    split_point = int(n_time_periods * 0.7)
    train_data = data[:, :, :split_point]
    test_data = data[:, :, split_point:]
    
    # Sauvegarder
    os.makedirs("data/processed", exist_ok=True)
    
    save_data(data, "data/processed/stock_data_normalized_latest.npy")
    save_data(train_data, "data/processed/stock_data_train_latest.npy")
    save_data(test_data, "data/processed/stock_data_test_latest.npy")
    
    print(f"âœ… DonnÃ©es crÃ©Ã©es: train={train_data.shape}, test={test_data.shape}")
    return True

def test_training_imports():
    """Test des imports nÃ©cessaires."""
    print("ğŸ“¦ Test des imports...")
    
    try:
        import mlflow
        print(f"   âœ… MLflow: {mlflow.__version__}")
        
        from stable_baselines3 import PPO
        print(f"   âœ… Stable-Baselines3: {PPO}")
        
        from src.models.train import train_portfolio_agent, load_training_data
        print("   âœ… Module d'entraÃ®nement local")
        
        from src.environment.portfolio_env import PortfolioEnv
        print("   âœ… Environnement RL")
        
        return True
        
    except ImportError as e:
        print(f"   âŒ Import Ã©chouÃ©: {e}")
        return False

def test_data_loading():
    """Test du chargement des donnÃ©es."""
    print("ğŸ“Š Test du chargement des donnÃ©es...")
    
    try:
        from src.models.train import load_training_data
        
        config_path = create_test_config()
        train_data, test_data = load_training_data(config_path)
        
        if train_data is not None and test_data is not None:
            print(f"   âœ… Train: {train_data.shape}")
            print(f"   âœ… Test: {test_data.shape}")
            os.unlink(config_path)
            return True
        else:
            print("   âŒ DonnÃ©es non chargÃ©es")
            os.unlink(config_path)
            return False
            
    except Exception as e:
        print(f"   âŒ Erreur: {e}")
        return False

def test_environment_creation():
    """Test de crÃ©ation de l'environnement."""
    print("ğŸ—ï¸ Test de crÃ©ation de l'environnement...")
    
    try:
        from src.environment.portfolio_env import create_env_from_config
        import numpy as np
        
        # DonnÃ©es factices
        test_data = np.random.rand(5, 3, 50).astype(np.float32)
        config_path = create_test_config()
        
        env = create_env_from_config(test_data, config_path)
        
        # Test basique
        obs, info = env.reset()
        action = env.action_space.sample()
        next_obs, reward, terminated, truncated, step_info = env.step(action)
        
        print("   âœ… Environnement crÃ©Ã© et testÃ©")
        os.unlink(config_path)
        return True
        
    except Exception as e:
        print(f"   âŒ Erreur: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_mlflow_connection():
    """Test de connexion Ã  MLflow."""
    print("ğŸ”— Test de connexion MLflow...")
    
    try:
        import mlflow
        
        # Configurer l'URI
        mlflow_uri = os.environ.get("MLFLOW_TRACKING_URI", "http://localhost:5000")
        mlflow.set_tracking_uri(mlflow_uri)
        
        # Test de connexion
        client = mlflow.tracking.MlflowClient()
        experiments = client.search_experiments()
        
        print(f"   âœ… Connexion MLflow rÃ©ussie ({len(experiments)} expÃ©riences)")
        return True
        
    except Exception as e:
        print(f"   âŒ Erreur MLflow: {e}")
        return False

def test_short_training():
    """Test d'un entraÃ®nement court."""
    print("ğŸ¯ Test d'entraÃ®nement court...")
    
    try:
        from src.models.train import train_portfolio_agent
        
        config_path = create_test_config()
        
        # EntraÃ®nement rapide
        print("   â³ DÃ©marrage de l'entraÃ®nement (5000 steps)...")
        model, training_info = train_portfolio_agent(
            config_path=config_path,
            run_name="test_training"
        )
        
        if model is not None and training_info is not None:
            print(f"   âœ… EntraÃ®nement rÃ©ussi!")
            print(f"      ğŸ“ Run ID: {training_info['run_id']}")
            print(f"      ğŸ† Meilleure rÃ©compense: {training_info['best_reward']:.4f}")
            print(f"      â±ï¸ Temps: {training_info['training_time']:.1f}s")
            
            # VÃ©rifier les fichiers crÃ©Ã©s
            final_model = training_info['final_model_path'] + ".zip"
            if os.path.exists(final_model):
                print(f"      âœ… ModÃ¨le sauvegardÃ©: {final_model}")
            else:
                print(f"      âš ï¸ ModÃ¨le non trouvÃ©: {final_model}")
            
            os.unlink(config_path)
            return True, training_info
        else:
            print("   âŒ EntraÃ®nement Ã©chouÃ©")
            os.unlink(config_path)
            return False, None
            
    except Exception as e:
        print(f"   âŒ Erreur: {e}")
        import traceback
        traceback.print_exc()
        return False, None

def test_model_evaluation():
    """Test d'Ã©valuation du modÃ¨le."""
    print("ğŸ“Š Test d'Ã©valuation du modÃ¨le...")
    
    try:
        from src.models.train import evaluate_trained_model
        
        # Utiliser le dernier modÃ¨le entraÃ®nÃ©
        config_path = create_test_config()
        
        # Chercher un modÃ¨le rÃ©cent
        logs_dir = "logs/training"
        if os.path.exists(logs_dir):
            training_dirs = [d for d in os.listdir(logs_dir) if d.startswith("training_")]
            if training_dirs:
                latest_dir = max(training_dirs)
                model_path = os.path.join(logs_dir, latest_dir, "final_model")
                
                if os.path.exists(model_path + ".zip"):
                    metrics = evaluate_trained_model(model_path, config_path)
                    
                    if metrics:
                        print("   âœ… Ã‰valuation rÃ©ussie:")
                        for key, value in metrics.items():
                            print(f"      {key}: {value:.4f}")
                        os.unlink(config_path)
                        return True
        
        print("   âš ï¸ Aucun modÃ¨le trouvÃ© pour l'Ã©valuation")
        os.unlink(config_path)
        return False
        
    except Exception as e:
        print(f"   âŒ Erreur: {e}")
        return False

def test_mlflow_ui_access():
    """Test d'accÃ¨s Ã  l'UI MLflow."""
    print("ğŸŒ Test d'accÃ¨s Ã  l'UI MLflow...")
    
    try:
        import requests
        
        mlflow_url = "http://localhost:5000"
        response = requests.get(mlflow_url, timeout=5)
        
        if response.status_code == 200:
            print(f"   âœ… UI MLflow accessible: {mlflow_url}")
            print(f"   ğŸŒ Ouvrez votre navigateur sur: {mlflow_url}")
            return True
        else:
            print(f"   âŒ UI MLflow non accessible (status: {response.status_code})")
            return False
            
    except Exception as e:
        print(f"   âŒ Erreur: {e}")
        return False

def cleanup_test_files():
    """Nettoie les fichiers de test."""
    print("ğŸ§¹ Nettoyage des fichiers de test...")
    
    files_to_clean = [
        "test_mlflow.db",
        "test_artifacts",
        "data/processed/stock_data_*_latest.npy"
    ]
    
    for pattern in files_to_clean:
        if "*" in pattern:
            import glob
            for file in glob.glob(pattern):
                try:
                    os.remove(file)
                    print(f"   ğŸ—‘ï¸ SupprimÃ©: {file}")
                except:
                    pass
        else:
            if os.path.exists(pattern):
                try:
                    if os.path.isdir(pattern):
                        import shutil
                        shutil.rmtree(pattern)
                    else:
                        os.remove(pattern)
                    print(f"   ğŸ—‘ï¸ SupprimÃ©: {pattern}")
                except:
                    pass

def main():
    """Fonction principale de test."""
    print("ğŸš€ Test complet du pipeline d'entraÃ®nement MLflow")
    print("=" * 60)
    
    # Variables de contrÃ´le
    mlflow_process = None
    cleanup_on_exit = True
    
    try:
        # CrÃ©er les rÃ©pertoires nÃ©cessaires
        os.makedirs("data/processed", exist_ok=True)
        os.makedirs("logs/training", exist_ok=True)
        os.makedirs("artifacts", exist_ok=True)
        
        # Tests sÃ©quentiels
        tests = [
            ("Imports nÃ©cessaires", test_training_imports),
            ("CrÃ©ation de donnÃ©es test", create_test_data),
            ("DÃ©marrage MLflow", lambda: start_mlflow_server()),
            ("Connexion MLflow", test_mlflow_connection),
            ("Chargement des donnÃ©es", test_data_loading),
            ("CrÃ©ation environnement", test_environment_creation),
            ("EntraÃ®nement court", test_short_training),
            ("Ã‰valuation modÃ¨le", test_model_evaluation),
            ("AccÃ¨s UI MLflow", test_mlflow_ui_access),
        ]
        
        results = []
        training_info = None
        
        for i, (test_name, test_func) in enumerate(tests):
            print(f"\n{'='*20} {i+1}. {test_name} {'='*20}")
            
            try:
                if test_name == "DÃ©marrage MLflow":
                    mlflow_process = test_func()
                    result = mlflow_process is not None
                    # Configurer la variable d'environnement
                    os.environ["MLFLOW_TRACKING_URI"] = "http://localhost:5000"
                elif test_name == "EntraÃ®nement court":
                    result, training_info = test_func()
                else:
                    result = test_func()
                
                results.append(result if result is not None else False)
                
                if not result and test_name in ["Imports nÃ©cessaires", "CrÃ©ation de donnÃ©es test"]:
                    print(f"âŒ Test critique Ã©chouÃ©: {test_name}")
                    break
                    
            except Exception as e:
                print(f"âŒ Test {test_name} Ã©chouÃ©: {e}")
                results.append(False)
        
        # RÃ©sumÃ© des rÃ©sultats
        print("\n" + "=" * 60)
        print("ğŸ“‹ RÃ‰SUMÃ‰ DES TESTS")
        print("=" * 60)
        
        passed = sum(1 for r in results if r)
        total = len(results)
        
        for i, (test_name, _) in enumerate(tests[:len(results)]):
            status = "âœ… PASSÃ‰" if results[i] else "âŒ Ã‰CHOUÃ‰"
            print(f"   {status}: {test_name}")
        
        print(f"\nğŸ“Š Score: {passed}/{total} tests passÃ©s ({passed/total*100:.1f}%)")
        
        if passed >= total - 1:  # Accepter 1 Ã©chec sur les tests non critiques
            print("\nğŸ‰ PIPELINE D'ENTRAÃNEMENT FONCTIONNEL!")
            print("\nğŸ“‹ Ce qui fonctionne:")
            print("   âœ… EntraÃ®nement avec MLflow")
            print("   âœ… Tracking des mÃ©triques")
            print("   âœ… Sauvegarde des modÃ¨les")
            print("   âœ… Ã‰valuation automatique")
            print("   âœ… Interface MLflow")
            
            if training_info:
                print(f"\nğŸ¯ DERNIÃˆRE SESSION D'ENTRAÃNEMENT:")
                print(f"   ğŸ“ Run ID: {training_info.get('run_id', 'N/A')}")
                print(f"   ğŸ† Meilleure rÃ©compense: {training_info.get('best_reward', 'N/A')}")
                print(f"   â±ï¸ Temps d'entraÃ®nement: {training_info.get('training_time', 'N/A')}s")
            
            print(f"\nğŸŒ MLflow UI: http://localhost:5000")
            print("\nğŸ“‹ Ã‰TAPE 4 VALIDÃ‰E!")
            print("ğŸ¯ Prochaine Ã©tape: API de prÃ©diction (Ã‰tape 5)")
            
            # Demander si on veut garder MLflow ouvert
            try:
                response = input("\nâ“ Garder MLflow ouvert pour exploration ? (y/N): ")
                if response.lower() in ['y', 'yes']:
                    cleanup_on_exit = False
                    print("ğŸ’¡ MLflow reste ouvert. Fermez manuellement quand terminÃ©.")
            except KeyboardInterrupt:
                print("\nâ¹ï¸ Interruption utilisateur")
        
        else:
            print(f"\nâš ï¸ {total-passed} test(s) ont Ã©chouÃ©.")
            print("VÃ©rifiez les erreurs ci-dessus avant de continuer.")
            return False
    
    except KeyboardInterrupt:
        print("\nâ¹ï¸ Tests interrompus par l'utilisateur")
        return False
    
    finally:
        # Nettoyage
        if cleanup_on_exit:
            if mlflow_process:
                print("\nğŸ›‘ ArrÃªt de MLflow...")
                mlflow_process.terminate()
                try:
                    mlflow_process.wait(timeout=5)
                except:
                    mlflow_process.kill()
            
            cleanup_test_files()
        else:
            print("\nğŸ’¡ MLflow toujours en cours. Pour l'arrÃªter:")
            print(f"   kill {mlflow_process.pid}")
    
    return True

if __name__ == "__main__":
    success = main()
    if not success:
        sys.exit(1)