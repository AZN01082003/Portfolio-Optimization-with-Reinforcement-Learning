"""
Pipeline MLOps complet pour Portfolio RL avec Airflow.
Orchestre l'ingestion, preprocessing, entra√Ænement, √©valuation et d√©ploiement.
"""
import os
import json
import logging
import subprocess
import sys
from datetime import datetime, timedelta
from typing import Dict, Any

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.http import SimpleHttpOperator
from airflow.operators.email import EmailOperator
from airflow.operators.bash import BashOperator
from airflow.sensors.http_sensor import HttpSensor
from airflow.sensors.filesystem import FileSensor
from airflow.utils.trigger_rule import TriggerRule

# Configuration
logger = logging.getLogger(__name__)

# Arguments par d√©faut
default_args = {
    'owner': 'mlops-team',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'max_active_runs': 1,
    'email': ['admin@portfolio-rl.com']  # √Ä adapter
}

# =============================================================================
# FONCTIONS DES T√ÇCHES
# =============================================================================

def check_data_quality(**context) -> bool:
    """V√©rifie la qualit√© des donn√©es avant traitement."""
    import numpy as np
    import pandas as pd
    from pathlib import Path
    
    logger.info("üîç V√©rification qualit√© des donn√©es")
    
    # V√©rifier les fichiers de donn√©es brutes
    data_dir = Path("/opt/airflow/data/raw")
    
    # Trouver le fichier le plus r√©cent
    data_files = list(data_dir.glob("stock_data_*.csv"))
    
    if not data_files:
        logger.error("‚ùå Aucun fichier de donn√©es trouv√©")
        return False
    
    latest_file = max(data_files, key=lambda f: f.stat().st_mtime)
    logger.info(f"üìÇ Fichier le plus r√©cent: {latest_file}")
    
    try:
        # Charger et v√©rifier
        df = pd.read_csv(latest_file, header=[0, 1], index_col=0)
        
        # V√©rifications de base
        if df.empty:
            logger.error("‚ùå DataFrame vide")
            return False
        
        if df.isnull().sum().sum() > len(df) * 0.1:  # Plus de 10% de NaN
            logger.error("‚ùå Trop de valeurs manquantes")
            return False
        
        # V√©rifier les colonnes attendues
        expected_features = ['Open', 'High', 'Low', 'Close', 'Volume']
        available_features = [col[0] for col in df.columns.get_level_values(0).unique()]
        
        missing_features = set(expected_features) - set(available_features)
        if missing_features:
            logger.error(f"‚ùå Features manquantes: {missing_features}")
            return False
        
        logger.info(f"‚úÖ Qualit√© donn√©es OK: {df.shape}")
        
        # Passer les m√©tadonn√©es au contexte
        context['task_instance'].xcom_push(
            key='data_info',
            value={
                'file_path': str(latest_file),
                'shape': list(df.shape),
                'features': available_features,
                'date_range': [str(df.index.min()), str(df.index.max())],
                'quality_score': 1.0 - (df.isnull().sum().sum() / df.size)
            }
        )
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erreur v√©rification: {e}")
        return False

def run_data_ingestion(**context) -> str:
    """Ex√©cute l'ingestion de donn√©es."""
    logger.info("üì• D√©marrage ingestion de donn√©es")
    
    try:
        # Ex√©cuter le script d'ingestion
        result = subprocess.run([
            sys.executable, "/opt/airflow/src/data/ingestion.py",
            "--config", "/opt/airflow/config/default.json"
        ], capture_output=True, text=True, timeout=600)
        
        if result.returncode == 0:
            logger.info("‚úÖ Ingestion r√©ussie")
            
            # Extraire les informations du stdout
            output_lines = result.stdout.split('\n')
            data_info = {}
            
            for line in output_lines:
                if "Donn√©es t√©l√©charg√©es:" in line:
                    data_info['downloaded'] = True
                elif "Fichier sauvegard√©:" in line:
                    data_info['file_path'] = line.split(":")[-1].strip()
            
            # Passer au contexte
            context['task_instance'].xcom_push(key='ingestion_result', value=data_info)
            
            return "success"
        else:
            logger.error(f"‚ùå Erreur ingestion: {result.stderr}")
            raise Exception(f"Ingestion failed: {result.stderr}")
            
    except subprocess.TimeoutExpired:
        logger.error("‚ùå Timeout ingestion")
        raise Exception("Ingestion timeout")
    except Exception as e:
        logger.error(f"‚ùå Erreur ingestion: {e}")
        raise

def run_preprocessing(**context) -> str:
    """Ex√©cute le preprocessing des donn√©es."""
    logger.info("üîÑ D√©marrage preprocessing")
    
    # R√©cup√©rer les infos de l'ingestion
    ingestion_info = context['task_instance'].xcom_pull(
        task_ids='data_ingestion', 
        key='ingestion_result'
    )
    
    logger.info(f"üìä Infos ingestion: {ingestion_info}")
    
    try:
        result = subprocess.run([
            sys.executable, "/opt/airflow/src/data/preprocessing.py",
            "--config", "/opt/airflow/config/default.json"
        ], capture_output=True, text=True, timeout=900)
        
        if result.returncode == 0:
            logger.info("‚úÖ Preprocessing r√©ussi")
            
            # Extraire les informations de sortie
            preprocessing_info = {
                'status': 'success',
                'processed_file': 'data/processed/features.npy',
                'labels_file': 'data/processed/labels.npy'
            }
            
            context['task_instance'].xcom_push(
                key='preprocessing_result', 
                value=preprocessing_info
            )
            
            return "success"
        else:
            logger.error(f"‚ùå Erreur preprocessing: {result.stderr}")
            raise Exception(f"Preprocessing failed: {result.stderr}")
            
    except subprocess.TimeoutExpired:
        logger.error("‚ùå Timeout preprocessing")
        raise Exception("Preprocessing timeout")
    except Exception as e:
        logger.error(f"‚ùå Erreur preprocessing: {e}")
        raise

def run_training(**context) -> str:
    """Ex√©cute l'entra√Ænement du mod√®le RL."""
    logger.info("ü§ñ D√©marrage entra√Ænement du mod√®le RL")
    
    # R√©cup√©rer les infos du preprocessing
    preprocessing_info = context['task_instance'].xcom_pull(
        task_ids='data_preprocessing',
        key='preprocessing_result'
    )
    
    logger.info(f"üìä Infos preprocessing: {preprocessing_info}")
    
    try:
        # Param√®tres d'entra√Ænement
        training_params = [
            "--config", "/opt/airflow/config/default.json",
            "--experiment-name", f"training_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            "--log-mlflow", "true"
        ]
        
        result = subprocess.run([
            sys.executable, "/opt/airflow/src/models/train.py"
        ] + training_params, capture_output=True, text=True, timeout=3600)
        
        if result.returncode == 0:
            logger.info("‚úÖ Entra√Ænement r√©ussi")
            
            # Extraire les m√©triques d'entra√Ænement
            training_info = {
                'status': 'success',
                'model_path': f"models/portfolio_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip",
                'experiment_name': f"training_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                'training_duration': 'extracted_from_logs'
            }
            
            # Parser les m√©triques du stdout
            output_lines = result.stdout.split('\n')
            for line in output_lines:
                if "Final reward:" in line:
                    training_info['final_reward'] = float(line.split(":")[-1].strip())
                elif "Model saved:" in line:
                    training_info['model_path'] = line.split(":")[-1].strip()
                elif "Training time:" in line:
                    training_info['training_duration'] = line.split(":")[-1].strip()
            
            context['task_instance'].xcom_push(
                key='training_result',
                value=training_info
            )
            
            return "success"
        else:
            logger.error(f"‚ùå Erreur entra√Ænement: {result.stderr}")
            raise Exception(f"Training failed: {result.stderr}")
            
    except subprocess.TimeoutExpired:
        logger.error("‚ùå Timeout entra√Ænement (1h)")
        raise Exception("Training timeout")
    except Exception as e:
        logger.error(f"‚ùå Erreur entra√Ænement: {e}")
        raise

def run_evaluation(**context) -> str:
    """Ex√©cute l'√©valuation du mod√®le entra√Æn√©."""
    logger.info("üìä D√©marrage √©valuation du mod√®le")
    
    # R√©cup√©rer les infos d'entra√Ænement
    training_info = context['task_instance'].xcom_pull(
        task_ids='model_training',
        key='training_result'
    )
    
    logger.info(f"ü§ñ Infos entra√Ænement: {training_info}")
    
    try:
        model_path = training_info.get('model_path', 'models/latest_model.zip')
        
        evaluation_params = [
            "--config", "/opt/airflow/config/default.json",
            "--model-path", model_path,
            "--evaluation-episodes", "100",
            "--save-results", "true"
        ]
        
        result = subprocess.run([
            sys.executable, "/opt/airflow/src/models/evaluate.py"
        ] + evaluation_params, capture_output=True, text=True, timeout=1800)
        
        if result.returncode == 0:
            logger.info("‚úÖ √âvaluation r√©ussie")
            
            # Parser les m√©triques d'√©valuation
            evaluation_metrics = {
                'status': 'success',
                'mean_reward': 0.0,
                'std_reward': 0.0,
                'sharpe_ratio': 0.0,
                'max_drawdown': 0.0,
                'total_return': 0.0,
                'win_rate': 0.0
            }
            
            output_lines = result.stdout.split('\n')
            for line in output_lines:
                if "Mean reward:" in line:
                    evaluation_metrics['mean_reward'] = float(line.split(":")[-1].strip())
                elif "Std reward:" in line:
                    evaluation_metrics['std_reward'] = float(line.split(":")[-1].strip())
                elif "Sharpe ratio:" in line:
                    evaluation_metrics['sharpe_ratio'] = float(line.split(":")[-1].strip())
                elif "Max drawdown:" in line:
                    evaluation_metrics['max_drawdown'] = float(line.split(":")[-1].strip())
                elif "Total return:" in line:
                    evaluation_metrics['total_return'] = float(line.split(":")[-1].strip())
                elif "Win rate:" in line:
                    evaluation_metrics['win_rate'] = float(line.split(":")[-1].strip())
            
            # V√©rifier les seuils de qualit√©
            quality_check = {
                'reward_threshold': evaluation_metrics['mean_reward'] > 0.1,
                'return_threshold': evaluation_metrics['total_return'] > 5.0,
                'sharpe_threshold': evaluation_metrics['sharpe_ratio'] > 0.5,
                'drawdown_threshold': evaluation_metrics['max_drawdown'] < 0.2
            }
            
            evaluation_metrics['quality_passed'] = all(quality_check.values())
            evaluation_metrics['quality_checks'] = quality_check
            
            context['task_instance'].xcom_push(
                key='evaluation_result',
                value=evaluation_metrics
            )
            
            # D√©cider si le mod√®le est bon pour le d√©ploiement
            if evaluation_metrics['quality_passed']:
                logger.info("‚úÖ Mod√®le valid√© pour d√©ploiement")
                return "deploy"
            else:
                logger.warning("‚ö†Ô∏è Mod√®le ne passe pas les crit√®res de qualit√©")
                return "retrain"
            
        else:
            logger.error(f"‚ùå Erreur √©valuation: {result.stderr}")
            raise Exception(f"Evaluation failed: {result.stderr}")
            
    except subprocess.TimeoutExpired:
        logger.error("‚ùå Timeout √©valuation (30min)")
        raise Exception("Evaluation timeout")
    except Exception as e:
        logger.error(f"‚ùå Erreur √©valuation: {e}")
        raise

def run_model_deployment(**context) -> str:
    """D√©ploie le mod√®le valid√©."""
    logger.info("üöÄ D√©marrage d√©ploiement du mod√®le")
    
    # R√©cup√©rer les infos d'√©valuation
    evaluation_info = context['task_instance'].xcom_pull(
        task_ids='model_evaluation',
        key='evaluation_result'
    )
    
    training_info = context['task_instance'].xcom_pull(
        task_ids='model_training',
        key='training_result'
    )
    
    logger.info(f"üìä M√©triques d'√©valuation: {evaluation_info}")
    
    try:
        if not evaluation_info.get('quality_passed', False):
            logger.warning("‚ö†Ô∏è Mod√®le non valid√© - arr√™t du d√©ploiement")
            return "skipped"
        
        model_path = training_info.get('model_path', 'models/latest_model.zip')
        
        # 1. Sauvegarder le mod√®le dans le registre MLflow
        deployment_params = [
            "--model-path", model_path,
            "--model-name", "portfolio_rl_model",
            "--stage", "Production",
            "--mlflow-uri", "http://mlflow:5000"
        ]
        
        result = subprocess.run([
            sys.executable, "/opt/airflow/src/models/mlflow_manager.py",
            "register-model"
        ] + deployment_params, capture_output=True, text=True, timeout=300)
        
        if result.returncode != 0:
            logger.error(f"‚ùå Erreur enregistrement MLflow: {result.stderr}")
            raise Exception("MLflow registration failed")
        
        # 2. D√©ployer sur l'API
        api_deployment_params = [
            "--model-name", "portfolio_rl_model",
            "--api-endpoint", "http://portfolio-api:8000/models/deploy",
            "--wait-ready", "true"
        ]
        
        result = subprocess.run([
            sys.executable, "/opt/airflow/src/api/deploy_model.py"
        ] + api_deployment_params, capture_output=True, text=True, timeout=300)
        
        if result.returncode == 0:
            logger.info("‚úÖ D√©ploiement r√©ussi")
            
            deployment_info = {
                'status': 'success',
                'model_name': 'portfolio_rl_model',
                'deployment_time': datetime.now().isoformat(),
                'model_version': training_info.get('experiment_name', 'unknown'),
                'api_endpoint': 'http://portfolio-api:8000',
                'mlflow_model_uri': f"models:/portfolio_rl_model/Production"
            }
            
            context['task_instance'].xcom_push(
                key='deployment_result',
                value=deployment_info
            )
            
            return "success"
        else:
            logger.error(f"‚ùå Erreur d√©ploiement API: {result.stderr}")
            raise Exception("API deployment failed")
            
    except subprocess.TimeoutExpired:
        logger.error("‚ùå Timeout d√©ploiement")
        raise Exception("Deployment timeout")
    except Exception as e:
        logger.error(f"‚ùå Erreur d√©ploiement: {e}")
        raise

def send_success_notification(**context) -> str:
    """Envoie une notification de succ√®s."""
    logger.info("üìß Envoi notification de succ√®s")
    
    # R√©cup√©rer toutes les informations du pipeline
    deployment_info = context['task_instance'].xcom_pull(
        task_ids='model_deployment',
        key='deployment_result'
    )
    
    evaluation_info = context['task_instance'].xcom_pull(
        task_ids='model_evaluation', 
        key='evaluation_result'
    )
    
    training_info = context['task_instance'].xcom_pull(
        task_ids='model_training',
        key='training_result'
    )
    
    # Cr√©er le rapport de succ√®s
    report = {
        'pipeline_status': 'SUCCESS',
        'execution_date': context['execution_date'].isoformat(),
        'training_metrics': {
            'final_reward': training_info.get('final_reward', 'N/A'),
            'training_duration': training_info.get('training_duration', 'N/A')
        },
        'evaluation_metrics': {
            'mean_reward': evaluation_info.get('mean_reward', 0),
            'sharpe_ratio': evaluation_info.get('sharpe_ratio', 0),
            'total_return': evaluation_info.get('total_return', 0),
            'quality_passed': evaluation_info.get('quality_passed', False)
        },
        'deployment_info': {
            'model_version': deployment_info.get('model_version', 'unknown'),
            'api_endpoint': deployment_info.get('api_endpoint', 'N/A')
        }
    }
    
    # Sauvegarder le rapport
    import json
    report_file = f"/opt/airflow/reports/pipeline_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    os.makedirs(os.path.dirname(report_file), exist_ok=True)
    
    with open(report_file, 'w') as f:
        json.dump(report, f, indent=2)
    
    logger.info(f"üìä Rapport sauvegard√©: {report_file}")
    
    # Log des m√©triques importantes
    logger.info("üéâ PIPELINE TERMIN√â AVEC SUCC√àS!")
    logger.info(f"   üìà Reward moyen: {evaluation_info.get('mean_reward', 'N/A')}")
    logger.info(f"   üí∞ Retour total: {evaluation_info.get('total_return', 'N/A')}%")
    logger.info(f"   üìä Sharpe ratio: {evaluation_info.get('sharpe_ratio', 'N/A')}")
    logger.info(f"   üöÄ Mod√®le d√©ploy√©: {deployment_info.get('model_version', 'N/A')}")
    
    return "success"

def send_failure_notification(**context) -> str:
    """Envoie une notification d'√©chec."""
    logger.error("üìß Envoi notification d'√©chec")
    
    # R√©cup√©rer les informations d'erreur
    failed_task = context.get('task_instance')
    exception = context.get('exception')
    
    failure_report = {
        'pipeline_status': 'FAILED',
        'execution_date': context['execution_date'].isoformat(),
        'failed_task': failed_task.task_id if failed_task else 'unknown',
        'error_message': str(exception) if exception else 'Unknown error',
        'dag_run_id': context.get('dag_run').run_id if context.get('dag_run') else 'unknown'
    }
    
    # Sauvegarder le rapport d'√©chec
    failure_file = f"/opt/airflow/reports/failure_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    os.makedirs(os.path.dirname(failure_file), exist_ok=True)
    
    with open(failure_file, 'w') as f:
        json.dump(failure_report, f, indent=2)
    
    logger.error(f"üí• Rapport d'√©chec sauvegard√©: {failure_file}")
    logger.error(f"üí• PIPELINE √âCHOU√â - T√¢che: {failure_report['failed_task']}")
    logger.error(f"üí• Erreur: {failure_report['error_message']}")
    
    return "failed"

def cleanup_old_models(**context) -> str:
    """Nettoie les anciens mod√®les et artifacts."""
    logger.info("üßπ Nettoyage des anciens mod√®les")
    
    try:
        # Nettoyer les mod√®les locaux (garder les 5 plus r√©cents)
        cleanup_script = """
import os
import glob
from pathlib import Path

models_dir = Path("/opt/airflow/models")
model_files = list(models_dir.glob("*.zip"))

# Trier par date de modification
model_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)

# Supprimer les anciens (garder les 5 plus r√©cents)
for old_model in model_files[5:]:
    try:
        old_model.unlink()
        print(f"Supprim√©: {old_model}")
    except Exception as e:
        print(f"Erreur suppression {old_model}: {e}")

print(f"Nettoyage termin√©. {len(model_files[5:])} mod√®les supprim√©s.")
"""
        
        result = subprocess.run([
            sys.executable, "-c", cleanup_script
        ], capture_output=True, text=True, timeout=60)
        
        if result.returncode == 0:
            logger.info("‚úÖ Nettoyage r√©ussi")
            logger.info(result.stdout)
        else:
            logger.warning(f"‚ö†Ô∏è Erreur nettoyage: {result.stderr}")
        
        return "success"
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Erreur nettoyage: {e}")
        return "partial"

def run_model_validation_tests(**context) -> str:
    """Ex√©cute des tests de validation sur le mod√®le d√©ploy√©."""
    logger.info("üß™ Tests de validation du mod√®le d√©ploy√©")
    
    try:
        # Tests de l'API
        test_params = [
            "--api-endpoint", "http://portfolio-api:8000",
            "--test-suite", "full",
            "--timeout", "300"
        ]
        
        result = subprocess.run([
            sys.executable, "/opt/airflow/src/tests/test_deployed_model.py"
        ] + test_params, capture_output=True, text=True, timeout=600)
        
        if result.returncode == 0:
            logger.info("‚úÖ Tests de validation r√©ussis")
            
            # Parser les r√©sultats des tests
            test_results = {
                'api_health': True,
                'model_prediction': True,
                'performance_baseline': True,
                'latency_check': True
            }
            
            context['task_instance'].xcom_push(
                key='validation_tests',
                value=test_results
            )
            
            return "success"
        else:
            logger.error(f"‚ùå Tests de validation √©chou√©s: {result.stderr}")
            return "failed"
            
    except subprocess.TimeoutExpired:
        logger.error("‚ùå Timeout tests de validation")
        return "timeout"
    except Exception as e:
        logger.error(f"‚ùå Erreur tests: {e}")
        return "error"

# =============================================================================
# D√âFINITION DU DAG
# =============================================================================

# Cr√©ation du DAG principal
dag = DAG(
    'portfolio_ml_pipeline',
    default_args=default_args,
    description='Pipeline MLOps complet pour Portfolio RL',
    schedule_interval='@daily',  # Ex√©cution quotidienne
    catchup=False,
    max_active_runs=1,
    tags=['mlops', 'portfolio', 'reinforcement-learning']
)

# =============================================================================
# D√âFINITION DES T√ÇCHES
# =============================================================================

# 1. V√©rification de la qualit√© des donn√©es
data_quality_check = PythonOperator(
    task_id='data_quality_check',
    python_callable=check_data_quality,
    dag=dag,
    retries=1
)

# 2. Ingestion des donn√©es
data_ingestion = PythonOperator(
    task_id='data_ingestion',
    python_callable=run_data_ingestion,
    dag=dag
)

# 3. Preprocessing des donn√©es
data_preprocessing = PythonOperator(
    task_id='data_preprocessing',
    python_callable=run_preprocessing,
    dag=dag
)

# 4. Entra√Ænement du mod√®le
model_training = PythonOperator(
    task_id='model_training',
    python_callable=run_training,
    dag=dag
)

# 5. √âvaluation du mod√®le
model_evaluation = PythonOperator(
    task_id='model_evaluation',
    python_callable=run_evaluation,
    dag=dag
)

# 6. D√©ploiement du mod√®le (conditionnel)
model_deployment = PythonOperator(
    task_id='model_deployment',
    python_callable=run_model_deployment,
    dag=dag
)

# 7. Tests de validation post-d√©ploiement
validation_tests = PythonOperator(
    task_id='validation_tests',
    python_callable=run_model_validation_tests,
    dag=dag
)

# 8. Nettoyage des anciens mod√®les
cleanup_models = PythonOperator(
    task_id='cleanup_models',
    python_callable=cleanup_old_models,
    dag=dag,
    trigger_rule=TriggerRule.ALL_DONE  # Ex√©cuter m√™me si d'autres t√¢ches √©chouent
)

# 9. Notification de succ√®s
success_notification = PythonOperator(
    task_id='success_notification',
    python_callable=send_success_notification,
    dag=dag,
    trigger_rule=TriggerRule.ALL_SUCCESS
)

# 10. Notification d'√©chec
failure_notification = PythonOperator(
    task_id='failure_notification',
    python_callable=send_failure_notification,
    dag=dag,
    trigger_rule=TriggerRule.ONE_FAILED
)

# 11. Capteur pour v√©rifier la disponibilit√© de l'API
api_health_sensor = HttpSensor(
    task_id='api_health_check',
    http_conn_id='portfolio_api',  # √Ä configurer dans Airflow
    endpoint='health',
    request_params={},
    poke_interval=30,
    timeout=300,
    dag=dag
)

# 12. Capteur pour v√©rifier la disponibilit√© de MLflow
mlflow_health_sensor = HttpSensor(
    task_id='mlflow_health_check',
    http_conn_id='mlflow_api',  # √Ä configurer dans Airflow
    endpoint='',
    request_params={},
    poke_interval=30,
    timeout=300,
    dag=dag
)

# =============================================================================
# D√âFINITION DES D√âPENDANCES
# =============================================================================

# Flux principal du pipeline
data_quality_check >> data_ingestion >> data_preprocessing >> model_training >> model_evaluation

# D√©ploiement conditionnel (seulement si √©valuation r√©ussie)
model_evaluation >> model_deployment >> validation_tests

# Notifications parall√®les
[validation_tests] >> success_notification
[data_quality_check, data_ingestion, data_preprocessing, model_training, model_evaluation, model_deployment, validation_tests] >> failure_notification

# Nettoyage en parall√®le
validation_tests >> cleanup_models

# Capteurs de sant√© en parall√®le (optionnels)
[api_health_sensor, mlflow_health_sensor] >> data_quality_check

# =============================================================================
# CONFIGURATION ADDITIONNELLE
# =============================================================================

# Variables de configuration pour le DAG
dag.doc_md = """
# Pipeline MLOps Portfolio RL

Ce pipeline orchestre l'ensemble du cycle de vie MLOps pour le syst√®me de trading par reinforcement learning.

## √âtapes du pipeline:

1. **V√©rification sant√© services**: V√©rifie que l'API et MLflow sont disponibles
2. **Contr√¥le qualit√© donn√©es**: Valide la qualit√© des donn√©es avant traitement
3. **Ingestion donn√©es**: T√©l√©charge les donn√©es financi√®res r√©centes
4. **Preprocessing**: Pr√©pare les donn√©es pour l'entra√Ænement
5. **Entra√Ænement**: Entra√Æne le mod√®le RL avec les nouvelles donn√©es
6. **√âvaluation**: √âvalue les performances du mod√®le entra√Æn√©
7. **D√©ploiement**: D√©ploie le mod√®le si les m√©triques sont satisfaisantes
8. **Tests validation**: V√©rifie le bon fonctionnement du mod√®le d√©ploy√©
9. **Nettoyage**: Supprime les anciens mod√®les pour √©conomiser l'espace
10. **Notifications**: Informe de l'√©tat du pipeline

## M√©triques surveill√©es:

- **Reward moyen**: Performance de trading
- **Sharpe ratio**: Ratio rendement/risque
- **Drawdown maximum**: Perte maximale
- **Taux de gain**: Pourcentage de trades gagnants

## Configuration:

Le pipeline peut √™tre configur√© via les variables d'environnement Airflow:
- `PORTFOLIO_CONFIG_PATH`: Chemin vers le fichier de configuration
- `MLFLOW_TRACKING_URI`: URI du serveur MLflow
- `API_ENDPOINT`: Point d'entr√©e de l'API Portfolio

## Fr√©quence:

Par d√©faut quotidienne, mais configurable selon les besoins.
"""

# Configuration des connexions par d√©faut
dag.params = {
    "config_path": "/opt/airflow/config/default.json",
    "mlflow_uri": "http://mlflow:5000",
    "api_endpoint": "http://portfolio-api:8000",
    "notification_email": "admin@portfolio-rl.com",
    "max_training_time": 3600,  # 1 heure
    "quality_thresholds": {
        "min_reward": 0.1,
        "min_return": 5.0,
        "min_sharpe": 0.5,
        "max_drawdown": 0.2
    }
}